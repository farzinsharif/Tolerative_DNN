{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yp0rVeJZOTYs",
        "outputId": "67687b8d-aac8-4623-b9f8-093928f82981"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:03<00:00, 43.9MB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⏳ Training VGG-16 for 20 epochs …\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20: 100%|██████████| 391/391 [00:30<00:00, 12.80it/s, loss=1.846]\n",
            "Epoch 2/20: 100%|██████████| 391/391 [00:26<00:00, 14.76it/s, loss=1.411]\n",
            "Epoch 3/20: 100%|██████████| 391/391 [00:26<00:00, 14.73it/s, loss=1.485]\n",
            "Epoch 4/20: 100%|██████████| 391/391 [00:26<00:00, 14.95it/s, loss=1.238]\n",
            "Epoch 5/20: 100%|██████████| 391/391 [00:26<00:00, 14.99it/s, loss=1.071]\n",
            "Epoch 6/20: 100%|██████████| 391/391 [00:26<00:00, 14.95it/s, loss=1.000]\n",
            "Epoch 7/20: 100%|██████████| 391/391 [00:25<00:00, 15.28it/s, loss=1.085]\n",
            "Epoch 8/20: 100%|██████████| 391/391 [00:25<00:00, 15.15it/s, loss=0.833]\n",
            "Epoch 9/20: 100%|██████████| 391/391 [00:25<00:00, 15.19it/s, loss=0.734]\n",
            "Epoch 10/20: 100%|██████████| 391/391 [00:26<00:00, 14.76it/s, loss=0.933]\n",
            "Epoch 11/20: 100%|██████████| 391/391 [00:27<00:00, 14.37it/s, loss=0.348]\n",
            "Epoch 12/20: 100%|██████████| 391/391 [00:26<00:00, 14.99it/s, loss=0.451]\n",
            "Epoch 13/20: 100%|██████████| 391/391 [00:25<00:00, 15.05it/s, loss=0.379]\n",
            "Epoch 14/20: 100%|██████████| 391/391 [00:25<00:00, 15.06it/s, loss=0.255]\n",
            "Epoch 15/20: 100%|██████████| 391/391 [00:25<00:00, 15.10it/s, loss=0.469]\n",
            "Epoch 16/20: 100%|██████████| 391/391 [00:26<00:00, 15.01it/s, loss=0.270]\n",
            "Epoch 17/20: 100%|██████████| 391/391 [00:26<00:00, 14.96it/s, loss=0.252]\n",
            "Epoch 18/20: 100%|██████████| 391/391 [00:26<00:00, 14.53it/s, loss=0.175]\n",
            "Epoch 19/20: 100%|██████████| 391/391 [00:26<00:00, 14.93it/s, loss=0.409]\n",
            "Epoch 20/20: 100%|██████████| 391/391 [00:26<00:00, 14.97it/s, loss=0.243]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Finished. Test accuracy after 20 epochs: **88.09 %** (checkpoint saved to model/vgg16_trained.pt)\n",
            "\n",
            "📊 Per-layer most-important filter (highest Σσ):\n",
            "  Layer  0 ➜ filter  18  (Σσ = 3.5622)\n",
            "  Layer  1 ➜ filter  61  (Σσ = 4.5687)\n",
            "  Layer  2 ➜ filter 124  (Σσ = 4.0786)\n",
            "  Layer  3 ➜ filter  14  (Σσ = 4.7922)\n",
            "  Layer  4 ➜ filter  48  (Σσ = 4.2364)\n",
            "  Layer  5 ➜ filter 224  (Σσ = 3.1613)\n",
            "  Layer  6 ➜ filter  27  (Σσ = 3.3195)\n",
            "  Layer  7 ➜ filter 195  (Σσ = 1.5316)\n",
            "  Layer  8 ➜ filter 296  (Σσ = 1.2354)\n",
            "  Layer  9 ➜ filter  49  (Σσ = 1.0492)\n",
            "  Layer 10 ➜ filter 247  (Σσ = 0.9704)\n",
            "  Layer 11 ➜ filter  80  (Σσ = 1.0230)\n",
            "  Layer 12 ➜ filter 311  (Σσ = 1.7227)\n",
            "\n",
            "📝 Full JSON written to: filter_importance.json\n"
          ]
        }
      ],
      "source": [
        "# ================================================================\n",
        "# VGG-16 on CIFAR-10  +  SVD-based Filter-Importance Ranking\n",
        "# ================================================================\n",
        "# ① install deps on Colab (comment-out when running locally)\n",
        "# !pip install --quiet torch torchvision tqdm\n",
        "\n",
        "import os, math, json, random, pathlib, torch, torch.nn as nn, torch.optim as optim\n",
        "import torchvision, torchvision.transforms as T\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ----------------------------- CONFIG ---------------------------\n",
        "NUM_EPOCHS        = 20\n",
        "BATCH_SIZE        = 128\n",
        "LR                = 0.1\n",
        "DEVICE            = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "RANDOM_SEED       = 42\n",
        "\n",
        "# output files\n",
        "CKPT_PATH         = \"model/vgg16_trained.pt\"\n",
        "SVD_JSON_PATH     = \"filter_importance.json\"\n",
        "\n",
        "# reproducibility\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "random.seed(RANDOM_SEED)\n",
        "os.makedirs(\"model\", exist_ok=True)\n",
        "\n",
        "# -------------------------- DATASET -----------------------------\n",
        "transform_train = T.Compose([\n",
        "    T.RandomHorizontalFlip(),\n",
        "    T.RandomCrop(32, padding=4),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "transform_test  = T.Compose([\n",
        "    T.ToTensor(),\n",
        "    T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,  download=True, transform=transform_train)\n",
        "testset  = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "train_loader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True,  num_workers=2, pin_memory=True)\n",
        "test_loader  = DataLoader(testset,  batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "# --------------------------- MODEL ------------------------------\n",
        "def make_vgg16():\n",
        "    vgg = torchvision.models.vgg16_bn(pretrained=False)\n",
        "    vgg.features[0] = nn.Conv2d(3, 64, kernel_size=3, padding=1)   # CIFAR adaptation\n",
        "    vgg.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "    vgg.classifier = nn.Sequential(\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(512, 512), nn.ReLU(True), nn.Dropout(),\n",
        "        nn.Linear(512, 512), nn.ReLU(True), nn.Dropout(),\n",
        "        nn.Linear(512, 10)\n",
        "    )\n",
        "    return vgg\n",
        "\n",
        "model = make_vgg16().to(DEVICE)\n",
        "\n",
        "# ---------------------- TRAIN / EVAL ----------------------------\n",
        "def accuracy(net, loader):\n",
        "    net.eval()\n",
        "    correct = total = 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "            preds = net(x).argmax(1)\n",
        "            correct += (preds == y).sum().item()\n",
        "            total   += y.size(0)\n",
        "    return 100. * correct / total\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=LR, momentum=0.9, weight_decay=5e-4)\n",
        "scheduler = optim.lr_scheduler.MultiStepLR(\n",
        "    optimizer, milestones=[NUM_EPOCHS//2, int(NUM_EPOCHS*0.75)], gamma=0.1)\n",
        "\n",
        "print(\"⏳ Training VGG-16 for 20 epochs …\")\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    model.train()\n",
        "    pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{NUM_EPOCHS}')\n",
        "    for imgs, lbls in pbar:\n",
        "        imgs, lbls = imgs.to(DEVICE), lbls.to(DEVICE)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss = criterion(model(imgs), lbls)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        pbar.set_postfix({'loss': f'{loss.item():.3f}'})\n",
        "    scheduler.step()\n",
        "\n",
        "torch.save(model.state_dict(), CKPT_PATH)\n",
        "test_acc = accuracy(model, test_loader)\n",
        "print(f'\\n✅ Finished. Test accuracy after 20 epochs: **{test_acc:.2f} %** (checkpoint saved to {CKPT_PATH})')\n",
        "\n",
        "# --------------- SVD-BASED FILTER IMPORTANCE (all filters) -------\n",
        "@torch.no_grad()\n",
        "def svd_rank_filters(net, json_path=SVD_JSON_PATH, preview_top_k=1):\n",
        "    \"\"\"\n",
        "    For each Conv2d layer:\n",
        "      • flatten each filter to (in_c, k*k)\n",
        "      • compute its singular values\n",
        "      • importance score = sum of singular values (nuclear norm)\n",
        "    Writes *all* filters to JSON.  Prints the top-k per layer for a quick look.\n",
        "    \"\"\"\n",
        "    conv_layers = [m for m in net.modules() if isinstance(m, nn.Conv2d)]\n",
        "    importance_records = []\n",
        "\n",
        "    print(\"\\nFilter-importance preview (top-{} per layer):\".format(preview_top_k))\n",
        "    for L, layer in enumerate(conv_layers):\n",
        "        layer_scores = []\n",
        "        for f_idx in range(layer.weight.size(0)):\n",
        "            w = layer.weight[f_idx].detach().cpu().reshape(layer.weight.size(1), -1)\n",
        "            svals = torch.linalg.svdvals(w)\n",
        "            score = svals.sum().item()\n",
        "            layer_scores.append((f_idx, score))\n",
        "\n",
        "            # store every filter\n",
        "            importance_records.append({\n",
        "                \"layer\"        : L,\n",
        "                \"filter_index\" : f_idx,\n",
        "                \"svd_score\"    : score\n",
        "            })\n",
        "\n",
        "        # console preview\n",
        "        layer_scores.sort(key=lambda x: x[1], reverse=True)\n",
        "        for rank, (f_idx, score) in enumerate(layer_scores[:preview_top_k], start=1):\n",
        "            print(f\"  Layer {L:2d}  Rank {rank} ➜ filter {f_idx:3d}   Σσ = {score:.4f}\")\n",
        "\n",
        "    with open(json_path, \"w\") as fp:\n",
        "        json.dump(importance_records, fp, indent=2)\n",
        "    print(f\"\\nFull JSON written to: {json_path}\")\n",
        "\n",
        "\n",
        "svd_rank_filters(model)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------- SVD-BASED FILTER IMPORTANCE (all filters) -------\n",
        "@torch.no_grad()\n",
        "def svd_rank_filters(net, json_path=SVD_JSON_PATH, preview_top_k=1):\n",
        "    \"\"\"\n",
        "    For each Conv2d layer:\n",
        "      • flatten each filter to (in_c, k*k)\n",
        "      • compute its singular values\n",
        "      • importance score = sum of singular values (nuclear norm)\n",
        "    Writes *all* filters to JSON.  Prints the top-k per layer for a quick look.\n",
        "    \"\"\"\n",
        "    conv_layers = [m for m in net.modules() if isinstance(m, nn.Conv2d)]\n",
        "    importance_records = []\n",
        "\n",
        "    print(\"\\nFilter-importance preview (top-{} per layer):\".format(preview_top_k))\n",
        "    for L, layer in enumerate(conv_layers):\n",
        "        layer_scores = []\n",
        "        for f_idx in range(layer.weight.size(0)):\n",
        "            w = layer.weight[f_idx].detach().cpu().reshape(layer.weight.size(1), -1)\n",
        "            svals = torch.linalg.svdvals(w)\n",
        "            score = svals.sum().item()\n",
        "            layer_scores.append((f_idx, score))\n",
        "\n",
        "            # store every filter\n",
        "            importance_records.append({\n",
        "                \"layer\"        : L,\n",
        "                \"filter_index\" : f_idx,\n",
        "                \"svd_score\"    : score\n",
        "            })\n",
        "\n",
        "        # console preview\n",
        "        layer_scores.sort(key=lambda x: x[1], reverse=True)\n",
        "        for rank, (f_idx, score) in enumerate(layer_scores[:preview_top_k], start=1):\n",
        "            print(f\"  Layer {L:2d}  Rank {rank} ➜ filter {f_idx:3d}   Σσ = {score:.4f}\")\n",
        "\n",
        "    with open(json_path, \"w\") as fp:\n",
        "        json.dump(importance_records, fp, indent=2)\n",
        "    print(f\"\\nFull JSON written to: {json_path}\")\n",
        "\n",
        "\n",
        "svd_rank_filters(model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4JOa3bvKSJx9",
        "outputId": "7bd7842b-993f-46a6-ab35-a8a4a29e8443"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Filter-importance preview (top-1 per layer):\n",
            "  Layer  0  Rank 1 ➜ filter  18   Σσ = 3.5622\n",
            "  Layer  1  Rank 1 ➜ filter  61   Σσ = 4.5687\n",
            "  Layer  2  Rank 1 ➜ filter 124   Σσ = 4.0786\n",
            "  Layer  3  Rank 1 ➜ filter  14   Σσ = 4.7922\n",
            "  Layer  4  Rank 1 ➜ filter  48   Σσ = 4.2364\n",
            "  Layer  5  Rank 1 ➜ filter 224   Σσ = 3.1613\n",
            "  Layer  6  Rank 1 ➜ filter  27   Σσ = 3.3195\n",
            "  Layer  7  Rank 1 ➜ filter 195   Σσ = 1.5316\n",
            "  Layer  8  Rank 1 ➜ filter 296   Σσ = 1.2354\n",
            "  Layer  9  Rank 1 ➜ filter  49   Σσ = 1.0492\n",
            "  Layer 10  Rank 1 ➜ filter 247   Σσ = 0.9704\n",
            "  Layer 11  Rank 1 ➜ filter  80   Σσ = 1.0230\n",
            "  Layer 12  Rank 1 ➜ filter 311   Σσ = 1.7227\n",
            "\n",
            "Full JSON written to: filter_importance.json\n"
          ]
        }
      ]
    }
  ]
}
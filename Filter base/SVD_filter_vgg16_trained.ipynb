{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yp0rVeJZOTYs",
        "outputId": "67687b8d-aac8-4623-b9f8-093928f82981"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170M/170M [00:03<00:00, 43.9MB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â³ Training VGG-16 for 20 epochs â€¦\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:30<00:00, 12.80it/s, loss=1.846]\n",
            "Epoch 2/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:26<00:00, 14.76it/s, loss=1.411]\n",
            "Epoch 3/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:26<00:00, 14.73it/s, loss=1.485]\n",
            "Epoch 4/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:26<00:00, 14.95it/s, loss=1.238]\n",
            "Epoch 5/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:26<00:00, 14.99it/s, loss=1.071]\n",
            "Epoch 6/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:26<00:00, 14.95it/s, loss=1.000]\n",
            "Epoch 7/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:25<00:00, 15.28it/s, loss=1.085]\n",
            "Epoch 8/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:25<00:00, 15.15it/s, loss=0.833]\n",
            "Epoch 9/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:25<00:00, 15.19it/s, loss=0.734]\n",
            "Epoch 10/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:26<00:00, 14.76it/s, loss=0.933]\n",
            "Epoch 11/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:27<00:00, 14.37it/s, loss=0.348]\n",
            "Epoch 12/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:26<00:00, 14.99it/s, loss=0.451]\n",
            "Epoch 13/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:25<00:00, 15.05it/s, loss=0.379]\n",
            "Epoch 14/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:25<00:00, 15.06it/s, loss=0.255]\n",
            "Epoch 15/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:25<00:00, 15.10it/s, loss=0.469]\n",
            "Epoch 16/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:26<00:00, 15.01it/s, loss=0.270]\n",
            "Epoch 17/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:26<00:00, 14.96it/s, loss=0.252]\n",
            "Epoch 18/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:26<00:00, 14.53it/s, loss=0.175]\n",
            "Epoch 19/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:26<00:00, 14.93it/s, loss=0.409]\n",
            "Epoch 20/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:26<00:00, 14.97it/s, loss=0.243]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "âœ… Finished. Test accuracy after 20 epochs: **88.09 %** (checkpoint saved to model/vgg16_trained.pt)\n",
            "\n",
            "ðŸ“Š Per-layer most-important filter (highest Î£Ïƒ):\n",
            "  Layer  0 âžœ filter  18  (Î£Ïƒ = 3.5622)\n",
            "  Layer  1 âžœ filter  61  (Î£Ïƒ = 4.5687)\n",
            "  Layer  2 âžœ filter 124  (Î£Ïƒ = 4.0786)\n",
            "  Layer  3 âžœ filter  14  (Î£Ïƒ = 4.7922)\n",
            "  Layer  4 âžœ filter  48  (Î£Ïƒ = 4.2364)\n",
            "  Layer  5 âžœ filter 224  (Î£Ïƒ = 3.1613)\n",
            "  Layer  6 âžœ filter  27  (Î£Ïƒ = 3.3195)\n",
            "  Layer  7 âžœ filter 195  (Î£Ïƒ = 1.5316)\n",
            "  Layer  8 âžœ filter 296  (Î£Ïƒ = 1.2354)\n",
            "  Layer  9 âžœ filter  49  (Î£Ïƒ = 1.0492)\n",
            "  Layer 10 âžœ filter 247  (Î£Ïƒ = 0.9704)\n",
            "  Layer 11 âžœ filter  80  (Î£Ïƒ = 1.0230)\n",
            "  Layer 12 âžœ filter 311  (Î£Ïƒ = 1.7227)\n",
            "\n",
            "ðŸ“ Full JSON written to: filter_importance.json\n"
          ]
        }
      ],
      "source": [
        "# ================================================================\n",
        "# VGG-16 on CIFAR-10  +  SVD-based Filter-Importance Ranking\n",
        "# ================================================================\n",
        "# â‘  install deps on Colab (comment-out when running locally)\n",
        "# !pip install --quiet torch torchvision tqdm\n",
        "\n",
        "import os, math, json, random, pathlib, torch, torch.nn as nn, torch.optim as optim\n",
        "import torchvision, torchvision.transforms as T\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ----------------------------- CONFIG ---------------------------\n",
        "NUM_EPOCHS        = 20\n",
        "BATCH_SIZE        = 128\n",
        "LR                = 0.1\n",
        "DEVICE            = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "RANDOM_SEED       = 42\n",
        "\n",
        "# output files\n",
        "CKPT_PATH         = \"model/vgg16_trained.pt\"\n",
        "SVD_JSON_PATH     = \"filter_importance.json\"\n",
        "\n",
        "# reproducibility\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "random.seed(RANDOM_SEED)\n",
        "os.makedirs(\"model\", exist_ok=True)\n",
        "\n",
        "# -------------------------- DATASET -----------------------------\n",
        "transform_train = T.Compose([\n",
        "    T.RandomHorizontalFlip(),\n",
        "    T.RandomCrop(32, padding=4),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "transform_test  = T.Compose([\n",
        "    T.ToTensor(),\n",
        "    T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,  download=True, transform=transform_train)\n",
        "testset  = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "train_loader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True,  num_workers=2, pin_memory=True)\n",
        "test_loader  = DataLoader(testset,  batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "# --------------------------- MODEL ------------------------------\n",
        "def make_vgg16():\n",
        "    vgg = torchvision.models.vgg16_bn(pretrained=False)\n",
        "    vgg.features[0] = nn.Conv2d(3, 64, kernel_size=3, padding=1)   # CIFAR adaptation\n",
        "    vgg.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "    vgg.classifier = nn.Sequential(\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(512, 512), nn.ReLU(True), nn.Dropout(),\n",
        "        nn.Linear(512, 512), nn.ReLU(True), nn.Dropout(),\n",
        "        nn.Linear(512, 10)\n",
        "    )\n",
        "    return vgg\n",
        "\n",
        "model = make_vgg16().to(DEVICE)\n",
        "\n",
        "# ---------------------- TRAIN / EVAL ----------------------------\n",
        "def accuracy(net, loader):\n",
        "    net.eval()\n",
        "    correct = total = 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "            preds = net(x).argmax(1)\n",
        "            correct += (preds == y).sum().item()\n",
        "            total   += y.size(0)\n",
        "    return 100. * correct / total\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=LR, momentum=0.9, weight_decay=5e-4)\n",
        "scheduler = optim.lr_scheduler.MultiStepLR(\n",
        "    optimizer, milestones=[NUM_EPOCHS//2, int(NUM_EPOCHS*0.75)], gamma=0.1)\n",
        "\n",
        "print(\"â³ Training VGG-16 for 20 epochs â€¦\")\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    model.train()\n",
        "    pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{NUM_EPOCHS}')\n",
        "    for imgs, lbls in pbar:\n",
        "        imgs, lbls = imgs.to(DEVICE), lbls.to(DEVICE)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss = criterion(model(imgs), lbls)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        pbar.set_postfix({'loss': f'{loss.item():.3f}'})\n",
        "    scheduler.step()\n",
        "\n",
        "torch.save(model.state_dict(), CKPT_PATH)\n",
        "test_acc = accuracy(model, test_loader)\n",
        "print(f'\\nâœ… Finished. Test accuracy after 20 epochs: **{test_acc:.2f} %** (checkpoint saved to {CKPT_PATH})')\n",
        "\n",
        "# --------------- SVD-BASED FILTER IMPORTANCE (all filters) -------\n",
        "@torch.no_grad()\n",
        "def svd_rank_filters(net, json_path=SVD_JSON_PATH, preview_top_k=1):\n",
        "    \"\"\"\n",
        "    For each Conv2d layer:\n",
        "      â€¢ flatten each filter to (in_c, k*k)\n",
        "      â€¢ compute its singular values\n",
        "      â€¢ importance score = sum of singular values (nuclear norm)\n",
        "    Writes *all* filters to JSON.  Prints the top-k per layer for a quick look.\n",
        "    \"\"\"\n",
        "    conv_layers = [m for m in net.modules() if isinstance(m, nn.Conv2d)]\n",
        "    importance_records = []\n",
        "\n",
        "    print(\"\\nFilter-importance preview (top-{} per layer):\".format(preview_top_k))\n",
        "    for L, layer in enumerate(conv_layers):\n",
        "        layer_scores = []\n",
        "        for f_idx in range(layer.weight.size(0)):\n",
        "            w = layer.weight[f_idx].detach().cpu().reshape(layer.weight.size(1), -1)\n",
        "            svals = torch.linalg.svdvals(w)\n",
        "            score = svals.sum().item()\n",
        "            layer_scores.append((f_idx, score))\n",
        "\n",
        "            # store every filter\n",
        "            importance_records.append({\n",
        "                \"layer\"        : L,\n",
        "                \"filter_index\" : f_idx,\n",
        "                \"svd_score\"    : score\n",
        "            })\n",
        "\n",
        "        # console preview\n",
        "        layer_scores.sort(key=lambda x: x[1], reverse=True)\n",
        "        for rank, (f_idx, score) in enumerate(layer_scores[:preview_top_k], start=1):\n",
        "            print(f\"  Layer {L:2d}  Rank {rank} âžœ filter {f_idx:3d}   Î£Ïƒ = {score:.4f}\")\n",
        "\n",
        "    with open(json_path, \"w\") as fp:\n",
        "        json.dump(importance_records, fp, indent=2)\n",
        "    print(f\"\\nFull JSON written to: {json_path}\")\n",
        "\n",
        "\n",
        "svd_rank_filters(model)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------- SVD-BASED FILTER IMPORTANCE (all filters) -------\n",
        "@torch.no_grad()\n",
        "def svd_rank_filters(net, json_path=SVD_JSON_PATH, preview_top_k=1):\n",
        "    \"\"\"\n",
        "    For each Conv2d layer:\n",
        "      â€¢ flatten each filter to (in_c, k*k)\n",
        "      â€¢ compute its singular values\n",
        "      â€¢ importance score = sum of singular values (nuclear norm)\n",
        "    Writes *all* filters to JSON.  Prints the top-k per layer for a quick look.\n",
        "    \"\"\"\n",
        "    conv_layers = [m for m in net.modules() if isinstance(m, nn.Conv2d)]\n",
        "    importance_records = []\n",
        "\n",
        "    print(\"\\nFilter-importance preview (top-{} per layer):\".format(preview_top_k))\n",
        "    for L, layer in enumerate(conv_layers):\n",
        "        layer_scores = []\n",
        "        for f_idx in range(layer.weight.size(0)):\n",
        "            w = layer.weight[f_idx].detach().cpu().reshape(layer.weight.size(1), -1)\n",
        "            svals = torch.linalg.svdvals(w)\n",
        "            score = svals.sum().item()\n",
        "            layer_scores.append((f_idx, score))\n",
        "\n",
        "            # store every filter\n",
        "            importance_records.append({\n",
        "                \"layer\"        : L,\n",
        "                \"filter_index\" : f_idx,\n",
        "                \"svd_score\"    : score\n",
        "            })\n",
        "\n",
        "        # console preview\n",
        "        layer_scores.sort(key=lambda x: x[1], reverse=True)\n",
        "        for rank, (f_idx, score) in enumerate(layer_scores[:preview_top_k], start=1):\n",
        "            print(f\"  Layer {L:2d}  Rank {rank} âžœ filter {f_idx:3d}   Î£Ïƒ = {score:.4f}\")\n",
        "\n",
        "    with open(json_path, \"w\") as fp:\n",
        "        json.dump(importance_records, fp, indent=2)\n",
        "    print(f\"\\nFull JSON written to: {json_path}\")\n",
        "\n",
        "\n",
        "svd_rank_filters(model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4JOa3bvKSJx9",
        "outputId": "7bd7842b-993f-46a6-ab35-a8a4a29e8443"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Filter-importance preview (top-1 per layer):\n",
            "  Layer  0  Rank 1 âžœ filter  18   Î£Ïƒ = 3.5622\n",
            "  Layer  1  Rank 1 âžœ filter  61   Î£Ïƒ = 4.5687\n",
            "  Layer  2  Rank 1 âžœ filter 124   Î£Ïƒ = 4.0786\n",
            "  Layer  3  Rank 1 âžœ filter  14   Î£Ïƒ = 4.7922\n",
            "  Layer  4  Rank 1 âžœ filter  48   Î£Ïƒ = 4.2364\n",
            "  Layer  5  Rank 1 âžœ filter 224   Î£Ïƒ = 3.1613\n",
            "  Layer  6  Rank 1 âžœ filter  27   Î£Ïƒ = 3.3195\n",
            "  Layer  7  Rank 1 âžœ filter 195   Î£Ïƒ = 1.5316\n",
            "  Layer  8  Rank 1 âžœ filter 296   Î£Ïƒ = 1.2354\n",
            "  Layer  9  Rank 1 âžœ filter  49   Î£Ïƒ = 1.0492\n",
            "  Layer 10  Rank 1 âžœ filter 247   Î£Ïƒ = 0.9704\n",
            "  Layer 11  Rank 1 âžœ filter  80   Î£Ïƒ = 1.0230\n",
            "  Layer 12  Rank 1 âžœ filter 311   Î£Ïƒ = 1.7227\n",
            "\n",
            "Full JSON written to: filter_importance.json\n"
          ]
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndrWQenca1Jq",
        "outputId": "e2bd8e60-7d57-40bb-dc6b-c435ad7efd0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/3: 100%|██████████| 391/391 [00:20<00:00, 18.97it/s, loss=1.397]\n",
            "Epoch 2/3: 100%|██████████| 391/391 [00:19<00:00, 19.72it/s, loss=1.051]\n",
            "Epoch 3/3: 100%|██████████| 391/391 [00:21<00:00, 18.16it/s, loss=0.882]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved checkpoint to /content/conv4_cifar10.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Baseline accuracy: 65.90 %\n",
            "\n",
            "Injecting random-re-draw fault into 10 % of all filters …\n",
            "\n",
            "Saved filter fault info to: filter_faults.json\n",
            "Preview of first few modified filters:\n",
            "\n",
            "Layer 0 | Filter 14\n",
            "Original filter weights: [[[-0.025583768263459206, -0.22822798788547516, 0.13870789110660553], [-0.19518321752548218, 0.03616151586174965, 0.17975522577762604], [0.004639085382223129, 0.11092764139175415, -0.2676488757133484]], [[-0.13350507616996765, 0.06978676468133926, 0.07439516484737396], [-0.046199340373277664, -0.028306545689702034, 0.21517762541770935], [0.15136569738388062, 0.0892552062869072, -0.1464972048997879]], [[-0.19809497892856598, 0.05169738829135895, 0.24041733145713806], [-0.08267105370759964, 0.03758010268211365, 0.27104777097702026], [0.11142636835575104, 0.09242378920316696, -0.06519197672605515]]]\n",
            "Modified filter weights: [[[0.009700939990580082, 0.10806868225336075, -0.008602511137723923], [0.04245300590991974, -0.09621994942426682, 0.03264927491545677], [-0.03247204050421715, -0.04087623581290245, 0.026398224756121635]], [[-0.06376749277114868, -0.08310631662607193, -0.01516568660736084], [-0.004628493916243315, 0.009961857460439205, -0.056021641939878464], [0.09288293123245239, -0.03572594374418259, 0.03440525755286217]], [[0.03984154388308525, -0.0016700938576832414, 0.07458659261465073], [-0.025825465098023415, -0.012704798951745033, 0.07373078167438507], [-0.016301866620779037, -0.05799813196063042, 0.11775654554367065]]]\n",
            "\n",
            "Layer 0 | Filter 1\n",
            "Original filter weights: [[[-0.15562079846858978, -0.05518508702516556, -0.26429682970046997], [0.11400466412305832, -0.2333100438117981, 0.038518279790878296], [-0.09680940955877304, -0.21630613505840302, 0.06792399287223816]], [[0.015289691276848316, 0.10643833130598068, -0.04208102822303772], [0.008818527683615685, 0.01901903562247753, -0.04720066860318184], [0.0022063252981752157, 0.01732000522315502, 0.10826783627271652]], [[-0.13015618920326233, 0.013339855708181858, -0.04518837109208107], [0.0837869942188263, -0.15833665430545807, -0.15515892207622528], [-0.15707004070281982, -0.2604823410511017, 0.15524184703826904]]]\n",
            "Modified filter weights: [[[0.006956877652555704, -0.0054108318872749805, -0.03587111458182335], [0.03783243149518967, 0.01857440359890461, -0.05024706944823265], [0.0004147367144469172, 0.016382895410060883, 0.014145401306450367]], [[-0.04462868347764015, -0.008131700567901134, -0.04031061753630638], [-0.0058408472687006, -0.08062080293893814, -0.007707484066486359], [-0.0032298830337822437, -0.026622025296092033, 0.0026643991004675627]], [[-0.0015680216019973159, -0.037156037986278534, -0.057908326387405396], [-0.0012457103002816439, -0.03792149946093559, -0.02078448049724102], [0.031944166868925095, -0.012725797481834888, -0.061520226299762726]]]\n",
            "\n",
            "Layer 0 | Filter 47\n",
            "Original filter weights: [[[-0.16422423720359802, -0.11780937761068344, -0.04467698186635971], [-0.06075425446033478, -0.03249487280845642, -0.17496620118618011], [0.23133398592472076, 0.13415521383285522, 0.10908403992652893]], [[-0.04732547327876091, -0.12300800532102585, -0.23456254601478577], [-0.12072040140628815, -0.14836935698986053, -0.16033309698104858], [-0.09715717285871506, 0.022160358726978302, 0.061349231749773026]], [[-0.10198323428630829, 0.015637142583727837, 0.08049824088811874], [0.2579101026058197, 0.18358615040779114, 0.2865118980407715], [0.10763264447450638, 0.2073875069618225, 0.2605450749397278]]]\n",
            "Modified filter weights: [[[-0.025937432423233986, 0.061337899416685104, 0.03127402812242508], [-0.045584239065647125, 0.02843417041003704, -0.016683276742696762], [0.0027896517422050238, -0.037647850811481476, -0.06858851760625839]], [[-0.003317333525046706, 0.049748193472623825, 0.07846232503652573], [0.0025671583134680986, -0.034333597868680954, -0.010891622863709927], [0.005239414982497692, -0.07806332409381866, -0.04209097474813461]], [[0.005674411542713642, -0.025411633774638176, 0.06173525005578995], [0.03183826059103012, -0.0042391750030219555, -0.07956065982580185], [0.04381508752703667, -0.010675969533622265, 0.08129199594259262]]]\n",
            "Post-fault accuracy: 47.60 %\n",
            "\n",
            "Accuracy drop: 18.30 percentage points\n"
          ]
        }
      ],
      "source": [
        "# ================================================================\n",
        "# Conv-4 on CIFAR-10 + Filter Fault Injection Benchmark (with JSON Logging)\n",
        "# ================================================================\n",
        "!pip install --quiet torch torchvision tqdm\n",
        "\n",
        "import math, random, os, pathlib, torch, torch.nn as nn, torch.optim as optim\n",
        "import torchvision, torchvision.transforms as T\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "\n",
        "# ----------------------------- CONFIG ---------------------------\n",
        "NUM_EPOCHS        = 3\n",
        "BATCH_SIZE        = 128\n",
        "LR                = 0.1\n",
        "DEVICE            = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "FAULT_PERCENT     = 10          # % of *all* filters to random-re-draw\n",
        "RANDOM_SEED       = 42\n",
        "PRETRAINED_PATH   = ''          # leave empty to train from scratch\n",
        "SAVE_CHECKPOINT_TO= '/content/conv4_cifar10.pt'\n",
        "\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "random.seed(RANDOM_SEED)\n",
        "\n",
        "# -------------------------- DATASET -----------------------------\n",
        "transform_train = T.Compose([\n",
        "    T.RandomHorizontalFlip(),\n",
        "    T.RandomCrop(32, padding=4),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "transform_test  = T.Compose([\n",
        "    T.ToTensor(),\n",
        "    T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "testset  = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "train_loader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True,  num_workers=2, pin_memory=True)\n",
        "test_loader  = DataLoader(testset,  batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "# --------------------------- MODEL ------------------------------\n",
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super().__init__()\n",
        "        self.block = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 64, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "    def forward(self, x): return self.block(x)\n",
        "\n",
        "class Conv4(nn.Module):\n",
        "    def __init__(self, num_classes=10, in_channels=3):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            ConvBlock(in_channels),\n",
        "            ConvBlock(64),\n",
        "            ConvBlock(64),\n",
        "            ConvBlock(64),\n",
        "        )\n",
        "        self.classifier = nn.Linear(64 * 2 * 2, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)    # flatten\n",
        "        return self.classifier(x)\n",
        "\n",
        "model = Conv4().to(DEVICE)\n",
        "\n",
        "# ---------------------- TRAIN / LOAD ----------------------------\n",
        "def accuracy(net, loader):\n",
        "    net.eval()\n",
        "    correct = total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in loader:\n",
        "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "            preds = net(images).argmax(1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total   += labels.size(0)\n",
        "    return 100. * correct / total\n",
        "\n",
        "if PRETRAINED_PATH and pathlib.Path(PRETRAINED_PATH).exists():\n",
        "    model.load_state_dict(torch.load(PRETRAINED_PATH, map_location=DEVICE))\n",
        "    print(f'Loaded pretrained weights from {PRETRAINED_PATH}')\n",
        "else:\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=LR, momentum=0.9, weight_decay=5e-4)\n",
        "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer,\n",
        "                    milestones=[NUM_EPOCHS//2, int(NUM_EPOCHS*0.75)], gamma=0.1)\n",
        "\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        model.train()\n",
        "        pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{NUM_EPOCHS}')\n",
        "        for images, labels in pbar:\n",
        "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            loss = criterion(model(images), labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            pbar.set_postfix({'loss': f'{loss.item():.3f}'})\n",
        "        scheduler.step()\n",
        "\n",
        "    torch.save(model.state_dict(), SAVE_CHECKPOINT_TO)\n",
        "    print(f'Saved checkpoint to {SAVE_CHECKPOINT_TO}')\n",
        "\n",
        "base_acc = accuracy(model, test_loader)\n",
        "print(f'\\nBaseline accuracy: {base_acc:5.2f} %')\n",
        "\n",
        "# ------------------ FILTER FAULT INJECTION ----------------------\n",
        "def redraw_filters(net, percent: float, sigma: float = 0.05, save_json_path='filter_faults.json'):\n",
        "    \"\"\"\n",
        "    Randomly re-draw `percent` % of filters from N(0, sigma²), evenly across Conv layers.\n",
        "    Save changed filters and their indices in a JSON file.\n",
        "    \"\"\"\n",
        "    conv_weights = [p for p in net.parameters() if p.ndim == 4]  # 4D conv filters\n",
        "    total_filters = sum(p.size(0) for p in conv_weights)\n",
        "    k = math.floor(total_filters * percent / 100 + 1e-6)\n",
        "\n",
        "    # Equal distribution across layers\n",
        "    per_layer = [math.floor(k / len(conv_weights))] * len(conv_weights)\n",
        "    for i in range(k - sum(per_layer)):\n",
        "        per_layer[i] += 1\n",
        "\n",
        "    torch.manual_seed(RANDOM_SEED)\n",
        "    changed_filters = []\n",
        "\n",
        "    for layer_idx, (p, n_fault) in enumerate(zip(conv_weights, per_layer)):\n",
        "        if n_fault == 0:\n",
        "            continue\n",
        "        idx_list = random.sample(range(p.size(0)), n_fault)\n",
        "        for idx in idx_list:\n",
        "            original = p[idx].detach().cpu().numpy().tolist()\n",
        "            noise = torch.randn_like(p[idx]) * sigma\n",
        "            p.data[idx] = noise\n",
        "            modified = p[idx].detach().cpu().numpy().tolist()\n",
        "            changed_filters.append({\n",
        "                \"layer\": layer_idx,\n",
        "                \"filter_index\": idx,\n",
        "                \"original_filter\": original,\n",
        "                \"modified_filter\": modified\n",
        "            })\n",
        "\n",
        "    # Save to JSON\n",
        "    with open(save_json_path, 'w') as f:\n",
        "        json.dump(changed_filters, f, indent=2)\n",
        "\n",
        "    print(f'\\nSaved filter fault info to: {save_json_path}')\n",
        "    print(f'Preview of first few modified filters:')\n",
        "    for entry in changed_filters[:3]:\n",
        "        print(f\"\\nLayer {entry['layer']} | Filter {entry['filter_index']}\")\n",
        "        print(\"Original filter weights:\", entry['original_filter'])\n",
        "        print(\"Modified filter weights:\", entry['modified_filter'])\n",
        "\n",
        "print(f'\\nInjecting random-re-draw fault into {FAULT_PERCENT} % of all filters …')\n",
        "redraw_filters(model, FAULT_PERCENT, save_json_path='filter_faults.json')\n",
        "faulty_acc = accuracy(model, test_loader)\n",
        "print(f'Post-fault accuracy: {faulty_acc:5.2f} %')\n",
        "\n",
        "print(f'\\nAccuracy drop: {base_acc - faulty_acc:5.2f} percentage points')\n"
      ]
    }
  ]
}
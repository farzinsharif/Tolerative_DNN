{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndrWQenca1Jq",
        "outputId": "649f5e5f-3fcd-4439-da86-d73f1a70078e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/farzin/anaconda3/envs/hp_dnn/lib/python3.6/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/20: 100%|██████████| 391/391 [01:04<00:00,  6.08it/s, loss=1.896]\n",
            "Epoch 2/20: 100%|██████████| 391/391 [00:59<00:00,  6.52it/s, loss=1.430]\n",
            "Epoch 3/20: 100%|██████████| 391/391 [01:00<00:00,  6.49it/s, loss=1.465]\n",
            "Epoch 4/20: 100%|██████████| 391/391 [01:00<00:00,  6.46it/s, loss=1.377]\n",
            "Epoch 5/20: 100%|██████████| 391/391 [01:00<00:00,  6.44it/s, loss=1.109]\n",
            "Epoch 6/20: 100%|██████████| 391/391 [01:01<00:00,  6.40it/s, loss=0.985]\n",
            "Epoch 7/20: 100%|██████████| 391/391 [01:01<00:00,  6.41it/s, loss=1.064]\n",
            "Epoch 8/20: 100%|██████████| 391/391 [01:01<00:00,  6.35it/s, loss=0.782]\n",
            "Epoch 9/20: 100%|██████████| 391/391 [01:01<00:00,  6.35it/s, loss=0.802]\n",
            "Epoch 10/20: 100%|██████████| 391/391 [01:01<00:00,  6.34it/s, loss=0.865]\n",
            "Epoch 11/20: 100%|██████████| 391/391 [01:01<00:00,  6.31it/s, loss=0.397]\n",
            "Epoch 12/20: 100%|██████████| 391/391 [01:02<00:00,  6.28it/s, loss=0.539]\n",
            "Epoch 13/20: 100%|██████████| 391/391 [01:02<00:00,  6.27it/s, loss=0.383]\n",
            "Epoch 14/20: 100%|██████████| 391/391 [01:02<00:00,  6.27it/s, loss=0.278]\n",
            "Epoch 15/20: 100%|██████████| 391/391 [01:02<00:00,  6.23it/s, loss=0.434]\n",
            "Epoch 16/20: 100%|██████████| 391/391 [01:02<00:00,  6.23it/s, loss=0.227]\n",
            "Epoch 17/20: 100%|██████████| 391/391 [01:03<00:00,  6.15it/s, loss=0.271]\n",
            "Epoch 18/20: 100%|██████████| 391/391 [01:03<00:00,  6.20it/s, loss=0.269]\n",
            "Epoch 19/20: 100%|██████████| 391/391 [01:03<00:00,  6.14it/s, loss=0.354]\n",
            "Epoch 20/20: 100%|██████████| 391/391 [01:03<00:00,  6.16it/s, loss=0.344]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved checkpoint to model/baseline_vgg16.pt\n",
            "\n",
            "Baseline accuracy: 87.78 %\n",
            "\n",
            "Injecting random-re-draw fault into 40 % of all filters …\n",
            "\n",
            "Saved filter fault info to: filter_faults.json\n",
            "Per-layer injection preview (one example each):\n",
            "\n",
            "Layer 0 | Filter 20\n",
            "Original[0][0][:3] → [0.05566065385937691, 0.11749757081270218, 0.171776682138443]\n",
            "Modified[0][0][:3] → [0.005019058473408222, -0.0600106418132782, -0.022671399638056755]\n",
            "\n",
            "Layer 1 | Filter 38\n",
            "Original[0][0][:3] → [0.004614957608282566, 0.008770661428570747, -0.011451121419668198]\n",
            "Modified[0][0][:3] → [-0.0697433203458786, 0.06225670501589775, -0.0775739997625351]\n",
            "\n",
            "Layer 2 | Filter 71\n",
            "Original[0][0][:3] → [0.015852827578783035, -0.0018034522654488683, 0.05568487197160721]\n",
            "Modified[0][0][:3] → [0.060690660029649734, 0.09165792912244797, -0.03767814487218857]\n",
            "\n",
            "Total layers affected: 3/13\n",
            "Total filters modified: 1682\n",
            "Post-fault accuracy: 10.00 %\n",
            "\n",
            "Accuracy drop: 77.78 percentage points\n"
          ]
        }
      ],
      "source": [
        "# ================================================================\n",
        "# VGG-16 on CIFAR-10 + Filter Fault Injection Benchmark (JSON Logging)\n",
        "# ================================================================\n",
        "# colab env\n",
        "#!pip install --quiet torch torchvision tqdm\n",
        "\n",
        "import math, random, os, pathlib, torch, torch.nn as nn, torch.optim as optim\n",
        "import torchvision, torchvision.transforms as T\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "\n",
        "# ----------------------------- CONFIG ---------------------------\n",
        "NUM_EPOCHS        = 20\n",
        "BATCH_SIZE        = 128\n",
        "LR                = 0.1\n",
        "DEVICE            = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "FAULT_PERCENT     = 40          # % of *all* filters to random-re-draw\n",
        "RANDOM_SEED       = 42\n",
        "PRETRAINED_PATH   = ''          # leave empty to train from scratch\n",
        "SAVE_CHECKPOINT_TO = \"model/baseline_vgg16.pt\"\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "random.seed(RANDOM_SEED)\n",
        "\n",
        "# -------------------------- DATASET -----------------------------\n",
        "transform_train = T.Compose([\n",
        "    T.RandomHorizontalFlip(),\n",
        "    T.RandomCrop(32, padding=4),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "transform_test  = T.Compose([\n",
        "    T.ToTensor(),\n",
        "    T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "testset  = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "train_loader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True,  num_workers=2, pin_memory=True)\n",
        "test_loader  = DataLoader(testset,  batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "# --------------------------- MODEL ------------------------------\n",
        "def make_vgg16():\n",
        "    # VGG-16 without pretraining\n",
        "    vgg16 = torchvision.models.vgg16_bn(pretrained=False)\n",
        "\n",
        "    # Replace first conv to adapt CIFAR-10 input (3x32x32)\n",
        "    vgg16.features[0] = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
        "\n",
        "    # Add Adaptive Pooling to get fixed output shape\n",
        "    vgg16.avgpool = nn.AdaptiveAvgPool2d((1, 1))  # Output will be [B, 512, 1, 1]\n",
        "\n",
        "    # Replace classifier\n",
        "    vgg16.classifier = nn.Sequential(\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(512, 512),\n",
        "        nn.ReLU(True),\n",
        "        nn.Dropout(),\n",
        "        nn.Linear(512, 512),\n",
        "        nn.ReLU(True),\n",
        "        nn.Dropout(),\n",
        "        nn.Linear(512, 10),  # 10 CIFAR-10 classes\n",
        "    )\n",
        "    return vgg16\n",
        "\n",
        "\n",
        "model = make_vgg16().to(DEVICE)\n",
        "\n",
        "# ---------------------- TRAIN / LOAD ----------------------------\n",
        "def accuracy(net, loader):\n",
        "    net.eval()\n",
        "    correct = total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in loader:\n",
        "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "            preds = net(images).argmax(1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total   += labels.size(0)\n",
        "    return 100. * correct / total\n",
        "\n",
        "if PRETRAINED_PATH and pathlib.Path(PRETRAINED_PATH).exists():\n",
        "    model.load_state_dict(torch.load(PRETRAINED_PATH, map_location=DEVICE))\n",
        "    print(f'Loaded pretrained weights from {PRETRAINED_PATH}')\n",
        "else:\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=LR, momentum=0.9, weight_decay=5e-4)\n",
        "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer,\n",
        "                    milestones=[NUM_EPOCHS//2, int(NUM_EPOCHS*0.75)], gamma=0.1)\n",
        "\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        model.train()\n",
        "        pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{NUM_EPOCHS}')\n",
        "        for images, labels in pbar:\n",
        "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            loss = criterion(model(images), labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            pbar.set_postfix({'loss': f'{loss.item():.3f}'})\n",
        "        scheduler.step()\n",
        "\n",
        "    torch.save(model.state_dict(), SAVE_CHECKPOINT_TO)\n",
        "    print(f'Saved checkpoint to {SAVE_CHECKPOINT_TO}')\n",
        "\n",
        "base_acc = accuracy(model, test_loader)\n",
        "print(f'\\nBaseline accuracy: {base_acc:5.2f} %')\n",
        "\n",
        "# ------------------ FILTER FAULT INJECTION ----------------------\n",
        "import json, math, random, torch\n",
        "from torch import nn\n",
        "\n",
        "def redraw_filters(\n",
        "    net,\n",
        "    percent: float,\n",
        "    sigma: float = 0.05,\n",
        "    save_json_path: str = \"content/filter_faults.json\",\n",
        "    deterministic: bool = False,   # ← set to True if you *want* repeatability\n",
        "):\n",
        "    \"\"\"\n",
        "    Redraw `percent` % of filters *in every Conv2d layer* from 𝒩(0,σ²).\n",
        "    Logs all changes to JSON and prints one example per layer.\n",
        "    \"\"\"\n",
        "    if deterministic:\n",
        "        random.seed(42)           # or any constant you like\n",
        "        torch.manual_seed(42)\n",
        "    else:\n",
        "        random.seed()             # system-time seed → different each run\n",
        "        torch.random.manual_seed(torch.randint(0, 2**31, ()).item())\n",
        "\n",
        "    changed_filters = []\n",
        "    conv_layers = [m for m in net.modules() if isinstance(m, nn.Conv2d)]\n",
        "\n",
        "    for layer_idx, layer in enumerate(conv_layers):\n",
        "        n_filters = layer.weight.size(0)\n",
        "        n_fault   = math.floor(n_filters * percent / 100 + 1e-6)\n",
        "        if n_fault == 0:                         # skip tiny layers\n",
        "            continue\n",
        "\n",
        "        idx_list = random.sample(range(n_filters), n_fault)\n",
        "\n",
        "        for idx in idx_list:\n",
        "            original = layer.weight[idx].detach().cpu().numpy().tolist()\n",
        "            noise    = torch.randn_like(layer.weight[idx]) * sigma\n",
        "            layer.weight.data[idx] = noise\n",
        "            modified = layer.weight[idx].detach().cpu().numpy().tolist()\n",
        "\n",
        "            changed_filters.append({\n",
        "                \"layer\"         : layer_idx,\n",
        "                \"filter_index\"  : idx,\n",
        "                \"original_filter\": original,\n",
        "                \"modified_filter\": modified,\n",
        "            })\n",
        "\n",
        "    # ---------- save ----------\n",
        "    with open(save_json_path, \"w\") as f:\n",
        "        json.dump(changed_filters, f, indent=2)\n",
        "\n",
        "    # ---------- nicer preview ----------\n",
        "    print(f\"\\nSaved filter fault info to: {save_json_path}\")\n",
        "    print(\"Per-layer injection preview (one example each):\")\n",
        "    seen = set()\n",
        "    for entry in changed_filters:\n",
        "        L = entry['layer']\n",
        "        if L in seen:                     # already showed one for this layer\n",
        "            continue\n",
        "        seen.add(L)\n",
        "        print(f\"\\nLayer {L} | Filter {entry['filter_index']}\")\n",
        "        print(\"Original[0][0][:3] →\", entry['original_filter'][0][0][:3])\n",
        "        print(\"Modified[0][0][:3] →\", entry['modified_filter'][0][0][:3])\n",
        "        if len(seen) == 3:                # don’t spam the console\n",
        "            break\n",
        "    print(f\"\\nTotal layers affected: {len(seen)}/{len(conv_layers)}\")\n",
        "    print(f\"Total filters modified: {len(changed_filters)}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(f'\\nInjecting random-re-draw fault into {FAULT_PERCENT} % of all filters …')\n",
        "redraw_filters(model, FAULT_PERCENT, save_json_path='filter_faults.json')\n",
        "faulty_acc = accuracy(model, test_loader)\n",
        "print(f'Post-fault accuracy: {faulty_acc:5.2f} %')\n",
        "\n",
        "print(f'\\nAccuracy drop: {base_acc - faulty_acc:5.2f} percentage points')\n",
        "# save the faulty model (badan line ro avaz kon)\n",
        "torch.save(model.state_dict(), \"model/faulty_vgg16.pt\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "df4AWjq4rSlp",
        "outputId": "46b11f69-c99c-458e-d1bd-9ee500069891"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Counter({7: 204,\n",
            "         8: 204,\n",
            "         9: 204,\n",
            "         10: 204,\n",
            "         11: 204,\n",
            "         12: 204,\n",
            "         4: 102,\n",
            "         5: 102,\n",
            "         6: 102,\n",
            "         2: 51,\n",
            "         3: 51,\n",
            "         0: 25,\n",
            "         1: 25})\n"
          ]
        }
      ],
      "source": [
        "import collections, json, pprint\n",
        "with open(\"filter_faults.json\") as f:\n",
        "    log = json.load(f)\n",
        "\n",
        "cnt = collections.Counter([e[\"layer\"] for e in log])\n",
        "pprint.pprint(cnt)          # prints {layer_idx: how_many_faults}\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "hp_dnn",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

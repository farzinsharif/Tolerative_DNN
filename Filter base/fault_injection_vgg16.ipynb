{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndrWQenca1Jq",
        "outputId": "649f5e5f-3fcd-4439-da86-d73f1a70078e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/2: 100%|██████████| 391/391 [00:25<00:00, 15.32it/s, loss=1.788]\n",
            "Epoch 2/2: 100%|██████████| 391/391 [00:26<00:00, 15.02it/s, loss=1.639]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved checkpoint to /content/vgg16_cifar10.pt\n",
            "\n",
            "Baseline accuracy: 35.25 %\n",
            "\n",
            "Injecting random-re-draw fault into 40 % of all filters …\n",
            "\n",
            "Saved filter fault info to: filter_faults.json\n",
            "Per-layer injection preview (one example each):\n",
            "\n",
            "Layer 0 | Filter 19\n",
            "Original[0][0][:3] → [0.12511463463306427, -0.17382930219173431, -0.10917522013187408]\n",
            "Modified[0][0][:3] → [0.04751596599817276, 0.08760388940572739, -0.02178332582116127]\n",
            "\n",
            "Layer 1 | Filter 51\n",
            "Original[0][0][:3] → [0.02161526121199131, 0.04777440056204796, 0.019324880093336105]\n",
            "Modified[0][0][:3] → [-0.05366959050297737, 0.009973062202334404, 0.016865497455000877]\n",
            "\n",
            "Layer 2 | Filter 25\n",
            "Original[0][0][:3] → [-0.05007331073284149, -0.036678120493888855, -0.005980122834444046]\n",
            "Modified[0][0][:3] → [0.06257182359695435, -0.010171408765017986, 0.021746372804045677]\n",
            "\n",
            "Total layers affected: 3/13\n",
            "Total filters modified: 1682\n",
            "Post-fault accuracy: 10.00 %\n",
            "\n",
            "Accuracy drop: 25.25 percentage points\n"
          ]
        }
      ],
      "source": [
        "# ================================================================\n",
        "# VGG-16 on CIFAR-10 + Filter Fault Injection Benchmark (JSON Logging)\n",
        "# ================================================================\n",
        "!pip install --quiet torch torchvision tqdm\n",
        "\n",
        "import math, random, os, pathlib, torch, torch.nn as nn, torch.optim as optim\n",
        "import torchvision, torchvision.transforms as T\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "\n",
        "# ----------------------------- CONFIG ---------------------------\n",
        "NUM_EPOCHS        = 2\n",
        "BATCH_SIZE        = 128\n",
        "LR                = 0.1\n",
        "DEVICE            = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "FAULT_PERCENT     = 40          # % of *all* filters to random-re-draw\n",
        "RANDOM_SEED       = 42\n",
        "PRETRAINED_PATH   = ''          # leave empty to train from scratch\n",
        "SAVE_CHECKPOINT_TO= '/content/vgg16_cifar10.pt'\n",
        "\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "random.seed(RANDOM_SEED)\n",
        "\n",
        "# -------------------------- DATASET -----------------------------\n",
        "transform_train = T.Compose([\n",
        "    T.RandomHorizontalFlip(),\n",
        "    T.RandomCrop(32, padding=4),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "transform_test  = T.Compose([\n",
        "    T.ToTensor(),\n",
        "    T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "testset  = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "train_loader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True,  num_workers=2, pin_memory=True)\n",
        "test_loader  = DataLoader(testset,  batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "# --------------------------- MODEL ------------------------------\n",
        "def make_vgg16():\n",
        "    # VGG-16 without pretraining\n",
        "    vgg16 = torchvision.models.vgg16_bn(weights=None)\n",
        "\n",
        "    # Replace first conv to adapt CIFAR-10 input (3x32x32)\n",
        "    vgg16.features[0] = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
        "\n",
        "    # Add Adaptive Pooling to get fixed output shape\n",
        "    vgg16.avgpool = nn.AdaptiveAvgPool2d((1, 1))  # Output will be [B, 512, 1, 1]\n",
        "\n",
        "    # Replace classifier\n",
        "    vgg16.classifier = nn.Sequential(\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(512, 512),\n",
        "        nn.ReLU(True),\n",
        "        nn.Dropout(),\n",
        "        nn.Linear(512, 512),\n",
        "        nn.ReLU(True),\n",
        "        nn.Dropout(),\n",
        "        nn.Linear(512, 10),  # 10 CIFAR-10 classes\n",
        "    )\n",
        "    return vgg16\n",
        "\n",
        "\n",
        "model = make_vgg16().to(DEVICE)\n",
        "\n",
        "# ---------------------- TRAIN / LOAD ----------------------------\n",
        "def accuracy(net, loader):\n",
        "    net.eval()\n",
        "    correct = total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in loader:\n",
        "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "            preds = net(images).argmax(1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total   += labels.size(0)\n",
        "    return 100. * correct / total\n",
        "\n",
        "if PRETRAINED_PATH and pathlib.Path(PRETRAINED_PATH).exists():\n",
        "    model.load_state_dict(torch.load(PRETRAINED_PATH, map_location=DEVICE))\n",
        "    print(f'Loaded pretrained weights from {PRETRAINED_PATH}')\n",
        "else:\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=LR, momentum=0.9, weight_decay=5e-4)\n",
        "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer,\n",
        "                    milestones=[NUM_EPOCHS//2, int(NUM_EPOCHS*0.75)], gamma=0.1)\n",
        "\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        model.train()\n",
        "        pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{NUM_EPOCHS}')\n",
        "        for images, labels in pbar:\n",
        "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            loss = criterion(model(images), labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            pbar.set_postfix({'loss': f'{loss.item():.3f}'})\n",
        "        scheduler.step()\n",
        "\n",
        "    torch.save(model.state_dict(), SAVE_CHECKPOINT_TO)\n",
        "    print(f'Saved checkpoint to {SAVE_CHECKPOINT_TO}')\n",
        "\n",
        "base_acc = accuracy(model, test_loader)\n",
        "print(f'\\nBaseline accuracy: {base_acc:5.2f} %')\n",
        "\n",
        "# ------------------ FILTER FAULT INJECTION ----------------------\n",
        "import json, math, random, torch\n",
        "from torch import nn\n",
        "\n",
        "def redraw_filters(\n",
        "    net,\n",
        "    percent: float,\n",
        "    sigma: float = 0.05,\n",
        "    save_json_path: str = \"filter_faults.json\",\n",
        "    deterministic: bool = False,   # ← set to True if you *want* repeatability\n",
        "):\n",
        "    \"\"\"\n",
        "    Redraw `percent` % of filters *in every Conv2d layer* from 𝒩(0,σ²).\n",
        "    Logs all changes to JSON and prints one example per layer.\n",
        "    \"\"\"\n",
        "    if deterministic:\n",
        "        random.seed(42)           # or any constant you like\n",
        "        torch.manual_seed(42)\n",
        "    else:\n",
        "        random.seed()             # system-time seed → different each run\n",
        "        torch.random.manual_seed(torch.randint(0, 2**31, ()).item())\n",
        "\n",
        "    changed_filters = []\n",
        "    conv_layers = [m for m in net.modules() if isinstance(m, nn.Conv2d)]\n",
        "\n",
        "    for layer_idx, layer in enumerate(conv_layers):\n",
        "        n_filters = layer.weight.size(0)\n",
        "        n_fault   = math.floor(n_filters * percent / 100 + 1e-6)\n",
        "        if n_fault == 0:                         # skip tiny layers\n",
        "            continue\n",
        "\n",
        "        idx_list = random.sample(range(n_filters), n_fault)\n",
        "\n",
        "        for idx in idx_list:\n",
        "            original = layer.weight[idx].detach().cpu().numpy().tolist()\n",
        "            noise    = torch.randn_like(layer.weight[idx]) * sigma\n",
        "            layer.weight.data[idx] = noise\n",
        "            modified = layer.weight[idx].detach().cpu().numpy().tolist()\n",
        "\n",
        "            changed_filters.append({\n",
        "                \"layer\"         : layer_idx,\n",
        "                \"filter_index\"  : idx,\n",
        "                \"original_filter\": original,\n",
        "                \"modified_filter\": modified,\n",
        "            })\n",
        "\n",
        "    # ---------- save ----------\n",
        "    with open(save_json_path, \"w\") as f:\n",
        "        json.dump(changed_filters, f, indent=2)\n",
        "\n",
        "    # ---------- nicer preview ----------\n",
        "    print(f\"\\nSaved filter fault info to: {save_json_path}\")\n",
        "    print(\"Per-layer injection preview (one example each):\")\n",
        "    seen = set()\n",
        "    for entry in changed_filters:\n",
        "        L = entry['layer']\n",
        "        if L in seen:                     # already showed one for this layer\n",
        "            continue\n",
        "        seen.add(L)\n",
        "        print(f\"\\nLayer {L} | Filter {entry['filter_index']}\")\n",
        "        print(\"Original[0][0][:3] →\", entry['original_filter'][0][0][:3])\n",
        "        print(\"Modified[0][0][:3] →\", entry['modified_filter'][0][0][:3])\n",
        "        if len(seen) == 3:                # don’t spam the console\n",
        "            break\n",
        "    print(f\"\\nTotal layers affected: {len(seen)}/{len(conv_layers)}\")\n",
        "    print(f\"Total filters modified: {len(changed_filters)}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(f'\\nInjecting random-re-draw fault into {FAULT_PERCENT} % of all filters …')\n",
        "redraw_filters(model, FAULT_PERCENT, save_json_path='filter_faults.json')\n",
        "faulty_acc = accuracy(model, test_loader)\n",
        "print(f'Post-fault accuracy: {faulty_acc:5.2f} %')\n",
        "\n",
        "print(f'\\nAccuracy drop: {base_acc - faulty_acc:5.2f} percentage points')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import collections, json, pprint\n",
        "with open(\"filter_faults.json\") as f:\n",
        "    log = json.load(f)\n",
        "\n",
        "cnt = collections.Counter([e[\"layer\"] for e in log])\n",
        "pprint.pprint(cnt)          # prints {layer_idx: how_many_faults}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "df4AWjq4rSlp",
        "outputId": "46b11f69-c99c-458e-d1bd-9ee500069891"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({7: 204,\n",
            "         8: 204,\n",
            "         9: 204,\n",
            "         10: 204,\n",
            "         11: 204,\n",
            "         12: 204,\n",
            "         4: 102,\n",
            "         5: 102,\n",
            "         6: 102,\n",
            "         2: 51,\n",
            "         3: 51,\n",
            "         0: 25,\n",
            "         1: 25})\n"
          ]
        }
      ]
    }
  ]
}
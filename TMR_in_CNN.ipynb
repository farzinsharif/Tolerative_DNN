{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WGfZzyNQTN7h",
        "outputId": "00be5bf8-97d0-4414-cbad-67ebf08f6f2a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/farzin/anaconda3/envs/hp_dnn/lib/python3.6/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.utils.prune as prune\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import pandas as pd\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "#training data\n",
        "train_data = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(train_data, batch_size=32, shuffle=True, num_workers=2)\n",
        "\n",
        "#test data\n",
        "test_data = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "testloader = torch.utils.data.DataLoader(test_data, batch_size=32, shuffle=False, num_workers=2)\n",
        "\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYhiGg65TXx9",
        "outputId": "0fa4a7c7-a8c4-4186-82fa-71617e4ff63c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "VGG(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (6): ReLU(inplace=True)\n",
              "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (8): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (10): ReLU(inplace=True)\n",
              "    (11): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (13): ReLU(inplace=True)\n",
              "    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (15): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (16): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (17): ReLU(inplace=True)\n",
              "    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (19): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (20): ReLU(inplace=True)\n",
              "    (21): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (22): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (23): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (24): ReLU(inplace=True)\n",
              "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (26): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (27): ReLU(inplace=True)\n",
              "    (28): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Dropout(p=0.5, inplace=False)\n",
              "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): Dropout(p=0.5, inplace=False)\n",
              "    (6): Linear(in_features=4096, out_features=100, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_save_name = 'VGG11_80.pt'\n",
        "path = F\"./model/VGG11/{model_save_name}\"\n",
        "AlexNet_model = torchvision.models.vgg11_bn(pretrained=True)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = AlexNet_model.to(device)\n",
        "model = torch.load(path, map_location=device)\n",
        "\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z03CMqcrTabn",
        "outputId": "f50cc2c4-3007-4772-d49f-e22158db0b9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch-ignite in /usr/local/lib/python3.11/dist-packages (0.5.2)\n",
            "Requirement already satisfied: torch<3,>=1.3 in /usr/local/lib/python3.11/dist-packages (from pytorch-ignite) (2.6.0+cu124)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from pytorch-ignite) (24.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.3->pytorch-ignite) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.3->pytorch-ignite) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.3->pytorch-ignite) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.3->pytorch-ignite) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.3->pytorch-ignite) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.3->pytorch-ignite) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.3->pytorch-ignite) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.3->pytorch-ignite) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.3->pytorch-ignite) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.3->pytorch-ignite) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.3->pytorch-ignite) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.3->pytorch-ignite) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.3->pytorch-ignite) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.3->pytorch-ignite) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.3->pytorch-ignite) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.3->pytorch-ignite) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.3->pytorch-ignite) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.3->pytorch-ignite) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.3->pytorch-ignite) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.3->pytorch-ignite) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=1.3->pytorch-ignite) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=1.3->pytorch-ignite) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "pip install pytorch-ignite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "NmjduZibTcwZ"
      },
      "outputs": [],
      "source": [
        "from ignite.metrics import Precision,Recall,Accuracy,ConfusionMatrix,TopKCategoricalAccuracy\n",
        "precision = Precision(device=device)\n",
        "#confusionMatrix=ConfusionMatrix(10,device=device)\n",
        "recall=Recall(device=device)\n",
        "acc=Accuracy(device=device)\n",
        "T_acc=TopKCategoricalAccuracy(k=5,device=device)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "sm = torch.nn.Softmax(dim=1)\n",
        "def test(model):\n",
        "    confi=0\n",
        "    sub_confi=0\n",
        "    correct=0\n",
        "    total=0\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for data in testloader:\n",
        "            images, labels = data\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            #loss = criterion(outputs, labels)\n",
        "            precision.update((outputs, labels))\n",
        "            #confusionMatrix.update((outputs, labels))\n",
        "            recall.update((outputs, labels))\n",
        "            acc.update((outputs, labels))\n",
        "            T_acc.update((outputs, labels))\n",
        "            probabilities = sm(outputs)\n",
        "            topk=torch.topk(probabilities, 1)\n",
        "            topk2=torch.topk(probabilities, 2)\n",
        "            cols = torch.chunk(topk2.values, 2, 1)\n",
        "            sub_confi+=(cols[0].sum()-cols[1].sum())/32\n",
        "            confi+=topk.values.sum()/32\n",
        "            index_conf=torch.nonzero(topk.values.reshape(-1)>0.50)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            correct += (predicted[index_conf] == labels[index_conf]).sum()\n",
        "            #print(topk.values)\n",
        "            total += labels.size(0)\n",
        "\n",
        "        return_acc=acc.compute()\n",
        "        return_pre=precision.compute()\n",
        "        return_rec=recall.compute()\n",
        "        return_tacc= T_acc.compute()\n",
        "        return_con=confi/len(testloader)\n",
        "        return_sub_con=sub_confi/len(testloader)\n",
        "        precision.reset()\n",
        "        recall.reset()\n",
        "        acc.reset()\n",
        "        #confusionMatrix.reset()\n",
        "        T_acc.reset()\n",
        "        acc_50=correct/total\n",
        "        return return_acc,return_pre,return_rec,return_tacc,return_con.item(),return_sub_con.item(),acc_50.item()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXz8YmTyTe1E",
        "outputId": "4d5579a5-8cdf-48c0-cae7-fc2215cf99a6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0.7475,\n",
              " tensor([0.9495, 0.8333, 0.7765, 0.6778, 0.4914, 0.7059, 0.8222, 0.7766, 0.9293,\n",
              "         0.8725, 0.7126, 0.5050, 0.8155, 0.7895, 0.7184, 0.7009, 0.7358, 0.8763,\n",
              "         0.7935, 0.7528, 0.9091, 0.7963, 0.7545, 0.8421, 0.7983, 0.6495, 0.7889,\n",
              "         0.5736, 0.8081, 0.6600, 0.7018, 0.7363, 0.7030, 0.6311, 0.7778, 0.4779,\n",
              "         0.7778, 0.7767, 0.6476, 0.8571, 0.7188, 0.8529, 0.7170, 0.7938, 0.7093,\n",
              "         0.6778, 0.5865, 0.6593, 0.8879, 0.7895, 0.5978, 0.8283, 0.5390, 0.8654,\n",
              "         0.7890, 0.5366, 0.8317, 0.8019, 0.8788, 0.7250, 0.8241, 0.8090, 0.6944,\n",
              "         0.6923, 0.6824, 0.6304, 0.7653, 0.6552, 0.8932, 0.8763, 0.7453, 0.7664,\n",
              "         0.4949, 0.6636, 0.5091, 0.8393, 0.8598, 0.7692, 0.7526, 0.7864, 0.6979,\n",
              "         0.7917, 0.8911, 0.7579, 0.7551, 0.8700, 0.8588, 0.8173, 0.8191, 0.8544,\n",
              "         0.7383, 0.8095, 0.7386, 0.6500, 0.8879, 0.6887, 0.7160, 0.7670, 0.5579,\n",
              "         0.8191], device='cuda:0', dtype=torch.float64),\n",
              " tensor([0.9400, 0.8500, 0.6600, 0.6100, 0.5700, 0.7200, 0.7400, 0.7300, 0.9200,\n",
              "         0.8900, 0.6200, 0.5100, 0.8400, 0.7500, 0.7400, 0.8200, 0.7800, 0.8500,\n",
              "         0.7300, 0.6700, 0.8000, 0.8600, 0.8300, 0.8000, 0.9500, 0.6300, 0.7100,\n",
              "         0.7400, 0.8000, 0.6600, 0.8000, 0.6700, 0.7100, 0.6500, 0.8400, 0.5400,\n",
              "         0.7700, 0.8000, 0.6800, 0.8400, 0.6900, 0.8700, 0.7600, 0.7700, 0.6100,\n",
              "         0.6100, 0.6100, 0.6000, 0.9500, 0.9000, 0.5500, 0.8200, 0.7600, 0.9000,\n",
              "         0.8600, 0.4400, 0.8400, 0.8500, 0.8700, 0.5800, 0.8900, 0.7200, 0.7500,\n",
              "         0.6300, 0.5800, 0.5800, 0.7500, 0.5700, 0.9200, 0.8500, 0.7900, 0.8200,\n",
              "         0.4900, 0.7100, 0.5600, 0.9400, 0.9200, 0.7000, 0.7300, 0.8100, 0.6700,\n",
              "         0.7600, 0.9000, 0.7200, 0.7400, 0.8700, 0.7300, 0.8500, 0.7700, 0.8800,\n",
              "         0.7900, 0.8500, 0.6500, 0.6500, 0.9500, 0.7300, 0.5800, 0.7900, 0.5300,\n",
              "         0.7700], device='cuda:0', dtype=torch.float64),\n",
              " 0.9463,\n",
              " 0.7619563341140747,\n",
              " 0.6550694704055786,\n",
              " 0.674299955368042)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# test(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "6f4wYg79ThPY"
      },
      "outputs": [],
      "source": [
        "# ================================================================\n",
        "# change number to bit representation\n",
        "# ================================================================\n",
        "import numpy as np\n",
        "\n",
        "def IEEE754_v2_tensor(numbers):\n",
        "\n",
        "    signs = np.where(numbers < 0, 1, 0)\n",
        "    numbers = np.abs(numbers)\n",
        "\n",
        "    int_parts = np.floor(numbers).astype(int)\n",
        "    dec_parts = numbers - int_parts\n",
        "\n",
        "    int_bin_parts = np.array([bin(x).replace('0b', '') if x > 0 else '' for x in int_parts])\n",
        "\n",
        "    mantissas = []\n",
        "    exponents = []\n",
        "\n",
        "    for i in range(len(numbers)):\n",
        "        if int_parts[i] > 0:\n",
        "            mantissa = int_bin_parts[i][1:] + fractional_to_bin(dec_parts[i], 23 - len(int_bin_parts[i][1:]))\n",
        "            exponent = len(int_bin_parts[i]) - 1\n",
        "        else:\n",
        "            fraction_bin = fractional_to_bin(dec_parts[i], 50)\n",
        "            first_one = fraction_bin.find('1')\n",
        "            exponent = -(first_one + 1)\n",
        "            mantissa = fraction_bin[first_one + 1:first_one + 24]\n",
        "\n",
        "        mantissa = (mantissa + '0' * 23)[:23]\n",
        "        mantissas.append(mantissa)\n",
        "        exponents.append(exponent)\n",
        "\n",
        "    exponents = np.array(exponents) + 127\n",
        "    exponent_bits = np.array([bin(e).replace('0b', '').zfill(8) for e in exponents])\n",
        "\n",
        "    ieee754_representations = np.array([\n",
        "        str(signs[i]) + exponent_bits[i] + mantissas[i] for i in range(len(numbers))\n",
        "    ])\n",
        "\n",
        "    return ieee754_representations\n",
        "\n",
        "\n",
        "def fractional_to_bin(dec_part, length=24):\n",
        "\n",
        "    mantissa = ''\n",
        "    for _ in range(length):\n",
        "        dec_part *= 2\n",
        "        int_part = int(dec_part)\n",
        "        mantissa += str(int_part)\n",
        "        dec_part -= int_part\n",
        "        if dec_part == 0:\n",
        "            break\n",
        "    return mantissa + '0' * (length - len(mantissa))  # Pad to ensure fixed length\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def inv_IEEE754_tensor(num_IEEE_array):\n",
        "\n",
        "    binary_matrix = np.array([list(num) for num in num_IEEE_array], dtype=int)\n",
        "\n",
        "\n",
        "    signs = binary_matrix[:, 0]\n",
        "\n",
        "    exponent_bits = binary_matrix[:, 1:9]\n",
        "    exponents = np.dot(exponent_bits, 2 ** np.arange(7, -1, -1))\n",
        "    mantissa_bits = binary_matrix[:, 9:].astype(float)\n",
        "    powers = 2.0 ** np.arange(-1, -mantissa_bits.shape[1] - 1, -1, dtype=float)\n",
        "    mantissas = np.dot(mantissa_bits, powers)\n",
        "    normalized_mantissas = 1.0 + mantissas\n",
        "    is_subnormal = (exponents == 0)\n",
        "    exponents = np.where(is_subnormal, -126, exponents - 127)\n",
        "    mantissas = np.where(is_subnormal, mantissas, normalized_mantissas)\n",
        "    is_zero = (exponents == -127) & (mantissa_bits.sum(axis=1) == 0)\n",
        "    numbers = mantissas * (2.0 ** exponents)\n",
        "    numbers = np.where(is_zero, 0.0, numbers)\n",
        "    numbers = np.where(signs == 1, -numbers, numbers)\n",
        "\n",
        "    return numbers\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "XF6364dBTjgC"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def bitFLIP_v3_tensor(original_values, positions_list):\n",
        "    original_values_np = original_values.cpu().detach().numpy()  # Convert Torch tensor to NumPy array\n",
        "\n",
        "    ieee_binary_strings = IEEE754_v2_tensor(original_values_np)  # Convert to IEEE 754 binary\n",
        "    flipped_binaries = []\n",
        "    for i, positions in enumerate(positions_list):\n",
        "        str_num = list(ieee_binary_strings[i])\n",
        "        for position in positions:\n",
        "            bit_position = 31 - position  # Convert to IEEE754 bit position\n",
        "            if bit_position == 1:  # Prevent flipping sign bit\n",
        "                bit_position = 0\n",
        "            #print(len(str_num))\n",
        "            str_num[bit_position] = '0' if str_num[bit_position] == '1' else '1'\n",
        "        if(original_values_np[i]==0):\n",
        "          #print(original_values_np[i])\n",
        "          str_num='00000000000000000000000000000000'\n",
        "        flipped_binaries.append(\"\".join(str_num))\n",
        "\n",
        "    flipped_values = inv_IEEE754_tensor(np.array(flipped_binaries))\n",
        "\n",
        "    flipped_values_tensor = torch.tensor(flipped_values, dtype=original_values.dtype, device=original_values.device, requires_grad=True)\n",
        "\n",
        "    return flipped_values_tensor\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "brnx4f09xZFl"
      },
      "outputs": [],
      "source": [
        "\"\"\" \n",
        "CODE BLOCK 1\n",
        "original code\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "import torch\n",
        "import random\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "def fault_tolerance_two_agree(BER, model):\n",
        "\n",
        "    t = torch.cat([param.view(-1) for name, param in model.named_parameters()\n",
        "                   if \"weight\" in name and \"norm\" not in name]).to('cuda:0')\n",
        "\n",
        "    count = len(t)\n",
        "    nums = int(count * 31 * BER)\n",
        "    print(f\"Total number of weights (float32): {count}\")\n",
        "\n",
        "# yani agar num = 6 bashe va count 20, ma az beyn 0 ta 620 byd 6 ta random pos entekhab konim :: Farzin\n",
        "# in this scenario count is 129167744 and the reason why is explained below\n",
        "# hala count * 31 be ma index bit haro mide\n",
        "    def random_bit_positions():\n",
        "        return random.sample(range(0, 31 * count), nums)\n",
        "    \n",
        "\n",
        "    pos1 = random_bit_positions()\n",
        "    pos2 = random_bit_positions()\n",
        "    pos3 = random_bit_positions()\n",
        "\n",
        "    # Step 3: Count occurrences across all three selections\n",
        "    all_positions = pos1 + pos2 + pos3\n",
        "    pos_counts = Counter(all_positions)\n",
        "\n",
        "    # Step 4: Keep positions that appear at least twice\n",
        "    two_or_more = [pos for pos, cnt in pos_counts.items() if cnt >= 2]\n",
        "\n",
        "    if len(two_or_more) == 0:\n",
        "        print(\"No 2-agreement bit flips found. Nothing to do.\")\n",
        "        return\n",
        "\n",
        "    lst_sorted_final = torch.tensor(sorted(two_or_more), device='cuda:0')\n",
        "\n",
        "\n",
        "    bit_positions = lst_sorted_final % 31\n",
        "    index_positions = (lst_sorted_final - bit_positions) // 31\n",
        "\n",
        "\n",
        "    bits_grouped = pd.DataFrame({\n",
        "        'index': index_positions.cpu(),\n",
        "        'bit': bit_positions.cpu()\n",
        "    }).groupby('index', sort=False)['bit'].apply(list).to_dict()\n",
        "\n",
        "    unique_indices = torch.tensor(list(bits_grouped.keys()), device='cuda:0')\n",
        "    bit_flips = [torch.tensor(bits_grouped[idx.item()], device='cuda:0') for idx in unique_indices]\n",
        "\n",
        "\n",
        "    mask = (t[unique_indices] != 0)\n",
        "    keep_idx = mask.nonzero(as_tuple=False).flatten()\n",
        "\n",
        "    if len(keep_idx) == 0:\n",
        "        print(\"No non-zero values to flip. Exiting.\")\n",
        "        return\n",
        "\n",
        "    unique_indices = unique_indices[keep_idx]\n",
        "    bit_flips = [bit_flips[i] for i in keep_idx.tolist()]\n",
        "\n",
        "    flipped_values = bitFLIP_v3_tensor(t[unique_indices], bit_flips)\n",
        "\n",
        "\n",
        "    start = 0\n",
        "    for name, param in model.named_parameters():\n",
        "        if \"weight\" in name and \"norm\" not in name:\n",
        "            param_size = param.numel()\n",
        "            end = start + param_size\n",
        "            mask = (unique_indices >= start) & (unique_indices < end)\n",
        "\n",
        "            if mask.any():\n",
        "                update_indices = unique_indices[mask] - start\n",
        "                param_flat = param.view(-1).clone()\n",
        "\n",
        "                non_zero_mask = param_flat[update_indices] != 0\n",
        "                if non_zero_mask.any():\n",
        "                    valid_update_indices = update_indices[non_zero_mask]\n",
        "                    valid_flipped_values = flipped_values[mask][non_zero_mask]\n",
        "                    param_flat[valid_update_indices] = valid_flipped_values\n",
        "\n",
        "                param.data.copy_(param_flat.view(param.shape))\n",
        "\n",
        "            start = end\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of weights (float32): 129167744\n",
            "200210\n",
            "200210\n",
            "200210\n",
            "600630\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>start</th>\n",
              "      <th>end</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>52731</td>\n",
              "      <td>53567</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>41013</td>\n",
              "      <td>41849</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3348</td>\n",
              "      <td>4184</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>18414</td>\n",
              "      <td>19250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>20088</td>\n",
              "      <td>20924</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>275</th>\n",
              "      <td>219537536</td>\n",
              "      <td>219680383</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>276</th>\n",
              "      <td>219680384</td>\n",
              "      <td>219823231</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>277</th>\n",
              "      <td>219966080</td>\n",
              "      <td>220108927</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>278</th>\n",
              "      <td>220108928</td>\n",
              "      <td>220251775</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>279</th>\n",
              "      <td>220251776</td>\n",
              "      <td>220394623</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>280 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         start        end\n",
              "0        52731      53567\n",
              "1        41013      41849\n",
              "2         3348       4184\n",
              "3        18414      19250\n",
              "4        20088      20924\n",
              "..         ...        ...\n",
              "275  219537536  219680383\n",
              "276  219680384  219823231\n",
              "277  219966080  220108927\n",
              "278  220108928  220251775\n",
              "279  220251776  220394623\n",
              "\n",
              "[280 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Positions repeated >=2 times: 30\n",
            "Positions inside valid range: 65395\n",
            "Positions satisfying both: 3\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "CODE BLOCK 2\n",
        "second edition :: Farzin\n",
        "\n",
        "in this block i try to show how many time condition of happening same position more than twice\n",
        "also check how many time the position are in the important ranges\n",
        "and then demonstrate how many time both scenarios happen toghther\n",
        "as for this rate of BER we have\n",
        "power = -4\n",
        "BER = 5 * (10 ** power)\n",
        "Positions repeated >=2 times: 2958\n",
        "Positions inside valid range: 660276\n",
        "Positions satisfying both: 298\n",
        "and for this:\n",
        "power = -5\n",
        "BER = 5 * (10 ** power)\n",
        "we have:\n",
        "Positions repeated >=2 times: 30\n",
        "Positions inside valid range: 65395\n",
        "Positions satisfying both: 3\n",
        "\n",
        "\"\"\"\n",
        "import torch\n",
        "import random\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import json\n",
        "# BER be soorate block akahar neveshte shode\n",
        "power = -5\n",
        "BER = 5 * (10 ** power)\n",
        "\n",
        "def fault_tolerance_two_agree(BER, model):\n",
        "    t = torch.cat([param.view(-1) for name, param in model.named_parameters()\n",
        "                   if \"weight\" in name and \"norm\" not in name]).to('cuda:0')\n",
        "\n",
        "    count = len(t)\n",
        "    nums = int(count * 31 * BER)\n",
        "    print(f\"Total number of weights (float32): {count}\")\n",
        "\n",
        "    def random_bit_positions():\n",
        "        return random.sample(range(0, 31 * count), nums)\n",
        "\n",
        "    pos1 = random_bit_positions()\n",
        "    pos2 = random_bit_positions()\n",
        "    pos3 = random_bit_positions()\n",
        "\n",
        "    # Optional print statements — comment these out if not needed\n",
        "    print(len(pos1))\n",
        "    print(len(pos2))\n",
        "    print(len(pos3))\n",
        "    print(len(pos1 + pos2 + pos3))  # or print(len(all_positions)) after definition\n",
        "\n",
        "    all_positions = pos1 + pos2 + pos3\n",
        "    pos_counts = Counter(all_positions)\n",
        "\n",
        "    # Load valid ranges from the JSON file\n",
        "    with open('content/filter_indices.json') as f:\n",
        "        range_data = json.load(f)\n",
        "\n",
        "    # Flatten and collect all valid index ranges\n",
        "    valid_ranges = []\n",
        "    for block in range_data.values():\n",
        "        for layer in block.values():\n",
        "            for bounds in layer.values():\n",
        "                valid_ranges.append(bounds)\n",
        "\n",
        "    # Display valid ranges as a DataFrame (optional)\n",
        "    display(pd.DataFrame(valid_ranges, columns=[\"start\", \"end\"]))  # comment this out to suppress output\n",
        "\n",
        "    def is_in_valid_range(index):\n",
        "        return any(start <= index <= end for start, end in valid_ranges)\n",
        "\n",
        "    # Debug counts after function is defined\n",
        "    count_repeat = sum(1 for pos, cnt in pos_counts.items() if cnt >= 2)\n",
        "    count_valid = sum(1 for pos in pos_counts if is_in_valid_range(pos // 31))\n",
        "    count_both = sum(1 for pos, cnt in pos_counts.items() if cnt >= 2 and is_in_valid_range(pos // 31))\n",
        "\n",
        "    print(f\"Positions repeated >=2 times: {count_repeat}\")\n",
        "    print(f\"Positions inside valid range: {count_valid}\")\n",
        "    print(f\"Positions satisfying both: {count_both}\")\n",
        "\n",
        "    # Apply both conditions\n",
        "    two_or_more = [pos for pos, cnt in pos_counts.items()\n",
        "                   if cnt >= 2 and is_in_valid_range(pos // 31)]\n",
        "\n",
        "    if len(two_or_more) == 0:\n",
        "        print(\"No 2-agreement bit flips found. Nothing to do.\")\n",
        "        return\n",
        "\n",
        "    lst_sorted_final = torch.tensor(sorted(two_or_more), device='cuda:0')\n",
        "\n",
        "    bit_positions = lst_sorted_final % 31\n",
        "    index_positions = (lst_sorted_final - bit_positions) // 31\n",
        "\n",
        "    bits_grouped = pd.DataFrame({\n",
        "        'index': index_positions.cpu(),\n",
        "        'bit': bit_positions.cpu()\n",
        "    }).groupby('index', sort=False)['bit'].apply(list).to_dict()\n",
        "\n",
        "    unique_indices = torch.tensor(list(bits_grouped.keys()), device='cuda:0')\n",
        "    bit_flips = [torch.tensor(bits_grouped[idx.item()], device='cuda:0') for idx in unique_indices]\n",
        "\n",
        "    mask = (t[unique_indices] != 0)\n",
        "    keep_idx = mask.nonzero(as_tuple=False).flatten()\n",
        "\n",
        "    if len(keep_idx) == 0:\n",
        "        print(\"No non-zero values to flip. Exiting.\")\n",
        "        return\n",
        "\n",
        "    unique_indices = unique_indices[keep_idx]\n",
        "    bit_flips = [bit_flips[i] for i in keep_idx.tolist()]\n",
        "\n",
        "    flipped_values = bitFLIP_v3_tensor(t[unique_indices], bit_flips)\n",
        "\n",
        "    start = 0\n",
        "    for name, param in model.named_parameters():\n",
        "        if \"weight\" in name and \"norm\" not in name:\n",
        "            param_size = param.numel()\n",
        "            end = start + param_size\n",
        "            mask = (unique_indices >= start) & (unique_indices < end)\n",
        "\n",
        "            if mask.any():\n",
        "                update_indices = unique_indices[mask] - start\n",
        "                param_flat = param.view(-1).clone()\n",
        "\n",
        "                non_zero_mask = param_flat[update_indices] != 0\n",
        "                if non_zero_mask.any():\n",
        "                    valid_update_indices = update_indices[non_zero_mask]\n",
        "                    valid_flipped_values = flipped_values[mask][non_zero_mask]\n",
        "                    param_flat[valid_update_indices] = valid_flipped_values\n",
        "\n",
        "                param.data.copy_(param_flat.view(param.shape))\n",
        "\n",
        "            start = end\n",
        "\n",
        "#  Call the function\n",
        "fault_tolerance_two_agree(BER, model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of weights (float32): 129167744\n",
            "2002100\n",
            "2002100\n",
            "2002100\n",
            "6006300\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>start</th>\n",
              "      <th>end</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>52731</td>\n",
              "      <td>53567</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>41013</td>\n",
              "      <td>41849</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3348</td>\n",
              "      <td>4184</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>18414</td>\n",
              "      <td>19250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>20088</td>\n",
              "      <td>20924</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>275</th>\n",
              "      <td>219537536</td>\n",
              "      <td>219680383</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>276</th>\n",
              "      <td>219680384</td>\n",
              "      <td>219823231</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>277</th>\n",
              "      <td>219966080</td>\n",
              "      <td>220108927</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>278</th>\n",
              "      <td>220108928</td>\n",
              "      <td>220251775</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>279</th>\n",
              "      <td>220251776</td>\n",
              "      <td>220394623</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>280 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         start        end\n",
              "0        52731      53567\n",
              "1        41013      41849\n",
              "2         3348       4184\n",
              "3        18414      19250\n",
              "4        20088      20924\n",
              "..         ...        ...\n",
              "275  219537536  219680383\n",
              "276  219680384  219823231\n",
              "277  219966080  220108927\n",
              "278  220108928  220251775\n",
              "279  220251776  220394623\n",
              "\n",
              "[280 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Positions repeated >=2 times: 2985\n",
            "Positions inside valid range: 658102\n",
            "Positions satisfying both: 329\n",
            "Random position fault injected not having TMR: 6000328\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-c088b4a8dbcc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    107\u001b[0m                 \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_flat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m \u001b[0mfault_tolerance_with_random_and_tmr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBER\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-10-c088b4a8dbcc>\u001b[0m in \u001b[0;36mfault_tolerance_with_random_and_tmr\u001b[0;34m(BER, model)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0mbits_grouped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbits_grouped_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0munique_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbits_grouped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cuda:0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m     \u001b[0mbit_flips\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbits_grouped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cuda:0'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0munique_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;31m# Filter out zero-value entries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-c088b4a8dbcc>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0mbits_grouped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbits_grouped_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0munique_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbits_grouped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cuda:0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m     \u001b[0mbit_flips\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbits_grouped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cuda:0'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0munique_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;31m# Filter out zero-value entries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "''' \n",
        "CODE BLOCK 3\n",
        "third edition after voice in telegram 5 shanbe asr :: Farzin\n",
        "'''\n",
        "\n",
        "import torch\n",
        "import random\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import json\n",
        "\n",
        "power = -4\n",
        "BER = 5 * (10 ** power)\n",
        "\n",
        "def fault_tolerance_with_random_and_tmr(BER, model):\n",
        "    t = torch.cat([param.view(-1) for name, param in model.named_parameters()\n",
        "                   if \"weight\" in name and \"norm\" not in name]).to('cuda:0')\n",
        "\n",
        "    count = len(t)\n",
        "    nums = int(count * 31 * BER)\n",
        "    print(f\"Total number of weights (float32): {count}\")\n",
        "\n",
        "    def random_bit_positions():\n",
        "        return random.sample(range(0, 31 * count), nums)\n",
        "\n",
        "    pos1 = random_bit_positions()\n",
        "    pos2 = random_bit_positions()\n",
        "    pos3 = random_bit_positions()\n",
        "\n",
        "    print(len(pos1))\n",
        "    print(len(pos2))\n",
        "    print(len(pos3))\n",
        "    print(len(pos1 + pos2 + pos3))\n",
        "\n",
        "    all_positions = pos1 + pos2 + pos3\n",
        "    pos_counts = Counter(all_positions)\n",
        "\n",
        "    # Load important index ranges\n",
        "    with open('content/filter_indices.json') as f:\n",
        "        range_data = json.load(f)\n",
        "\n",
        "    valid_ranges = []\n",
        "    for block in range_data.values():\n",
        "        for layer in block.values():\n",
        "            for bounds in layer.values():\n",
        "                valid_ranges.append(bounds)\n",
        "\n",
        "    display(pd.DataFrame(valid_ranges, columns=[\"start\", \"end\"]))\n",
        "\n",
        "    def is_in_valid_range(index):\n",
        "        return any(start <= index <= end for start, end in valid_ranges)\n",
        "\n",
        "    # === Diagnostics ===\n",
        "    count_repeat = sum(1 for pos, cnt in pos_counts.items() if cnt >= 2)\n",
        "    count_valid = sum(1 for pos in pos_counts if is_in_valid_range(pos // 31))\n",
        "    count_both = sum(1 for pos, cnt in pos_counts.items() if cnt >= 2 and is_in_valid_range(pos // 31))\n",
        "    print(f\"Positions repeated >=2 times: {count_repeat}\")\n",
        "    print(f\"Positions inside valid range: {count_valid}\")\n",
        "    print(f\"Positions satisfying both: {count_both}\")\n",
        "\n",
        "    # === New Logic: inject all sampled positions ===\n",
        "    lst_sorted_final = torch.tensor(sorted(set(all_positions)), device='cuda:0')\n",
        "    bit_positions = lst_sorted_final % 31\n",
        "    index_positions = (lst_sorted_final - bit_positions) // 31\n",
        "\n",
        "    # TMR map\n",
        "    is_tmr = torch.tensor([pos_counts[pos.item()] >= 2 for pos in lst_sorted_final], device='cuda:0')\n",
        "    num_non_tmr = (~is_tmr).sum().item()\n",
        "    print(f\"Random position fault injected not having TMR: {num_non_tmr}\")\n",
        "\n",
        "    # Group bits per index\n",
        "    bits_grouped_df = pd.DataFrame({\n",
        "        'index': index_positions.cpu(),\n",
        "        'bit': bit_positions.cpu()\n",
        "    }).groupby('index', sort=False)['bit'].apply(list)\n",
        "\n",
        "    bits_grouped = bits_grouped_df.to_dict()\n",
        "    unique_indices = torch.tensor(list(bits_grouped.keys()), device='cuda:0')\n",
        "    bit_flips = [torch.tensor(bits_grouped[idx.item()], device='cuda:0') for idx in unique_indices]\n",
        "\n",
        "    # Filter out zero-value entries\n",
        "    mask = (t[unique_indices] != 0)\n",
        "    keep_idx = mask.nonzero(as_tuple=False).flatten()\n",
        "\n",
        "    if len(keep_idx) == 0:\n",
        "        print(\"No non-zero values to flip. Exiting.\")\n",
        "        return\n",
        "\n",
        "    unique_indices = unique_indices[keep_idx]\n",
        "    bit_flips = [bit_flips[i] for i in keep_idx.tolist()]\n",
        "\n",
        "    flipped_values = bitFLIP_v3_tensor(t[unique_indices], bit_flips)\n",
        "\n",
        "    # Inject the flipped values back into model\n",
        "    start = 0\n",
        "    for name, param in model.named_parameters():\n",
        "        if \"weight\" in name and \"norm\" not in name:\n",
        "            param_size = param.numel()\n",
        "            end = start + param_size\n",
        "            mask = (unique_indices >= start) & (unique_indices < end)\n",
        "\n",
        "            if mask.any():\n",
        "                update_indices = unique_indices[mask] - start\n",
        "                param_flat = param.view(-1).clone()\n",
        "                non_zero_mask = param_flat[update_indices] != 0\n",
        "\n",
        "                if non_zero_mask.any():\n",
        "                    valid_update_indices = update_indices[non_zero_mask]\n",
        "                    valid_flipped_values = flipped_values[mask][non_zero_mask]\n",
        "                    param_flat[valid_update_indices] = valid_flipped_values\n",
        "\n",
        "                param.data.copy_(param_flat.view(param.shape))\n",
        "            start = end\n",
        "fault_tolerance_with_random_and_tmr(BER, model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "CODE BLOCK 4\n",
        "\"\"\"\n",
        "import torch\n",
        "import random\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import json\n",
        "# BER be soorate block akahar neveshte shode\n",
        "power = -5\n",
        "BER = 5 * (10 ** power)\n",
        "\n",
        "def fault_tolerance_two_agree(BER, model):\n",
        "    t = torch.cat([param.view(-1) for name, param in model.named_parameters()\n",
        "                   if \"weight\" in name and \"norm\" not in name]).to('cuda:0')\n",
        "\n",
        "    count = len(t)\n",
        "    nums = int(count * 31 * BER)\n",
        "    # print(f\"Total number of weights (float32): {count}\")\n",
        "\n",
        "    def random_bit_positions():\n",
        "        return random.sample(range(0, 31 * count), nums)\n",
        "\n",
        "    pos1 = random_bit_positions()\n",
        "    pos2 = random_bit_positions()\n",
        "    pos3 = random_bit_positions()\n",
        "\n",
        "    # Optional print statements — comment these out if not needed\n",
        "    # print(len(pos1))\n",
        "    # print(len(pos2))\n",
        "    # print(len(pos3))\n",
        "    # print(len(pos1 + pos2 + pos3))  # or print(len(all_positions)) after definition\n",
        "\n",
        "    all_positions = pos1 + pos2 + pos3\n",
        "    pos_counts = Counter(all_positions)\n",
        "\n",
        "    # Load valid ranges from the JSON file\n",
        "    with open('content/filter_indices.json') as f:\n",
        "        range_data = json.load(f)\n",
        "\n",
        "    # Flatten and collect all valid index ranges\n",
        "    valid_ranges = []\n",
        "    for block in range_data.values():\n",
        "        for layer in block.values():\n",
        "            for bounds in layer.values():\n",
        "                valid_ranges.append(bounds)\n",
        "\n",
        "    # Display valid ranges as a DataFrame (optional)\n",
        "    # display(pd.DataFrame(valid_ranges, columns=[\"start\", \"end\"]))  # comment this out to suppress output\n",
        "\n",
        "    def is_in_valid_range(index):\n",
        "        return any(start <= index <= end for start, end in valid_ranges)\n",
        "\n",
        "    # Debug counts after function is defined\n",
        "    count_repeat = sum(1 for pos, cnt in pos_counts.items() if cnt >= 2)\n",
        "    count_valid = sum(1 for pos in pos_counts if is_in_valid_range(pos // 31))\n",
        "    count_both = sum(1 for pos, cnt in pos_counts.items() if cnt >= 2 and is_in_valid_range(pos // 31))\n",
        "\n",
        "    print(f\"Positions repeated >=2 times: {count_repeat}\")\n",
        "    print(f\"Positions inside valid range: {count_valid}\")\n",
        "    print(f\"Positions satisfying both: {count_both}\")\n",
        "\n",
        "    # Apply both conditions\n",
        "    two_or_more = [pos for pos, cnt in pos_counts.items()\n",
        "                   if cnt >= 2 and is_in_valid_range(pos // 31)] #Only implement tmr if in the certain values\n",
        "    random_pos_in_pos1 = [x for x in pos1 if not is_in_valid_range(x // 31)] # Not TMR fault injection\n",
        " \n",
        "    all_pos = two_or_more + random_pos_in_pos1\n",
        "    # print(len(all_pos))\n",
        "    # print(len(two_or_more))\n",
        "    if len(two_or_more) == 0:\n",
        "        print(\"No 2-agreement bit flips found. Nothing to do.\")\n",
        "        return\n",
        "\n",
        "    lst_sorted_final = torch.tensor(sorted(all_pos), device='cuda:0')\n",
        "\n",
        "    bit_positions = lst_sorted_final % 31\n",
        "    index_positions = (lst_sorted_final - bit_positions) // 31\n",
        "\n",
        "    bits_grouped = pd.DataFrame({\n",
        "        'index': index_positions.cpu(),\n",
        "        'bit': bit_positions.cpu()\n",
        "    }).groupby('index', sort=False)['bit'].apply(list).to_dict()\n",
        "\n",
        "    unique_indices = torch.tensor(list(bits_grouped.keys()), device='cuda:0')\n",
        "    bit_flips = [torch.tensor(bits_grouped[idx.item()], device='cuda:0') for idx in unique_indices]\n",
        "\n",
        "    mask = (t[unique_indices] != 0)\n",
        "    keep_idx = mask.nonzero(as_tuple=False).flatten()\n",
        "\n",
        "    if len(keep_idx) == 0:\n",
        "        print(\"No non-zero values to flip. Exiting.\")\n",
        "        return\n",
        "\n",
        "    unique_indices = unique_indices[keep_idx]\n",
        "    bit_flips = [bit_flips[i] for i in keep_idx.tolist()]\n",
        "\n",
        "    flipped_values = bitFLIP_v3_tensor(t[unique_indices], bit_flips)\n",
        "\n",
        "    start = 0\n",
        "    for name, param in model.named_parameters():\n",
        "        if \"weight\" in name and \"norm\" not in name:\n",
        "            param_size = param.numel()\n",
        "            end = start + param_size\n",
        "            mask = (unique_indices >= start) & (unique_indices < end)\n",
        "\n",
        "            if mask.any():\n",
        "                update_indices = unique_indices[mask] - start\n",
        "                param_flat = param.view(-1).clone()\n",
        "\n",
        "                non_zero_mask = param_flat[update_indices] != 0\n",
        "                if non_zero_mask.any():\n",
        "                    valid_update_indices = update_indices[non_zero_mask]\n",
        "                    valid_flipped_values = flipped_values[mask][non_zero_mask]\n",
        "                    param_flat[valid_update_indices] = valid_flipped_values\n",
        "\n",
        "                param.data.copy_(param_flat.view(param.shape))\n",
        "\n",
        "            start = end\n",
        "\n",
        "#  Call the function\n",
        "# fault_tolerance_two_agree(BER, model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of weights (float32): 129167744\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "129167744"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\" \n",
        "Test block to see the total weight\n",
        "Note: more explanation of the weights are demonstrated in the md below:: Farzin\n",
        "\"\"\"\n",
        "\n",
        "def count_eligible_weights(model):\n",
        "    # Flatten and concatenate all eligible weight parameters\n",
        "    t = torch.cat([param.view(-1) for name, param in model.named_parameters()\n",
        "                   if \"weight\" in name and \"norm\" not in name])\n",
        "    count = len(t)\n",
        "    print(f\"Total number of weights (float32): {count}\")\n",
        "    return count\n",
        "count_eligible_weights(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Conv Layers      ≈     9,217,728\n",
        "BatchNorm Layers ≈         2,752\n",
        "FC Layers        ≈   119,947,264\n",
        "-------------------------------\n",
        "Total            ≈   129,167,744\n",
        "\n",
        "as for wiehg i mean every elemnt is filter matrix is counted as weight.\n",
        "\n",
        "Total for each block:\n",
        "\n",
        "Conv(3→64): 64×3×3×3 = 1,728\n",
        "\n",
        "Conv(64→128): 128×64×3×3 = 73,728\n",
        "\n",
        "Conv(128→256): 256×128×3×3 = 294,912\n",
        "\n",
        "Conv(256→256): 256×256×3×3 = 589,824\n",
        "\n",
        "Conv(256→512): 512×256×3×3 = 1,179,648\n",
        "\n",
        "Conv(512→512): 512×512×3×3 = 2,359,296\n",
        "\n",
        "Conv(512→512): same = 2,359,296\n",
        "\n",
        "Conv(512→512): same = 2,359,296\n",
        "\n",
        "Total Conv Weights: ≈ 9,217,728\n",
        "\n",
        "BatchNorm Layers:\n",
        "Each BatchNorm2d(N) has:\n",
        "\n",
        "weight and bias: 2 × N\n",
        "\n",
        "So total:\n",
        "\n",
        "64 + 128 + 256×2 + 512×4 = 64 + 128 + 512 + 2048 = 2752 parameters\n",
        "\n",
        "Fully Connected Layers:\n",
        "Linear(25088 → 4096): 25088×4096 = 102,760,448\n",
        "\n",
        "Linear(4096 → 4096): 4096×4096 = 16,777,216\n",
        "\n",
        "Linear(4096 → 100): 4096×100 = 409,600\n",
        "\n",
        "Total FC Weights: ≈ 119,947,264\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of weights (float32): 129167744\n",
            "Number of bit positions to flip (per sample): 40042\n",
            "\n",
            "Sample of pos1 (first 20 values):\n",
            "[32551699, 3461382910, 148570343, 2259859158, 3971755526, 3645883447, 1796423017, 1099535078, 3395408415, 2209980651, 1142917107, 767363203, 3743073789, 649478275, 2817776700, 3227970557, 2491188087, 1290860242, 823729090, 1935931036]\n",
            "\n",
            "Sample of pos2 (first 20 values):\n",
            "[2537623569, 1280341706, 2783037484, 1842018405, 3542952018, 423145824, 1180314512, 1665003637, 1386941032, 1784913626, 3842959508, 3686969653, 1865132059, 481196669, 726682983, 2191985411, 1833387996, 3201864521, 3056644947, 541395872]\n",
            "\n",
            "Sample of pos3 (first 20 values):\n",
            "[2172207273, 3509702377, 2573835359, 3531194268, 1744900925, 2147815851, 1790275129, 741085325, 3319359032, 2994136826, 3795606568, 908169583, 826849407, 138487784, 2297079265, 2975785173, 1359128736, 873246472, 2938691339, 920139150]\n",
            "\n",
            "Total combined positions (len=120126), sample:\n",
            "[32551699, 3461382910, 148570343, 2259859158, 3971755526, 3645883447, 1796423017, 1099535078, 3395408415, 2209980651, 1142917107, 767363203, 3743073789, 649478275, 2817776700, 3227970557, 2491188087, 1290860242, 823729090, 1935931036]\n",
            "40042\n",
            "40042\n",
            "40042\n",
            "120126\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "test block to see the strucure of the position::Farzin\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import random\n",
        "from collections import Counter\n",
        "BER = 1e-5\n",
        "\n",
        "def debug_bit_positions(BER, model):\n",
        "    # Flatten weights (excluding normalization layers)\n",
        "    t = torch.cat([\n",
        "        param.detach().cpu().view(-1)\n",
        "        for name, param in model.named_parameters()\n",
        "        if \"weight\" in name and \"norm\" not in name\n",
        "    ])\n",
        "    \n",
        "    count = len(t)\n",
        "    nums = int(count * 31 * BER)\n",
        "    \n",
        "    print(f\"Total number of weights (float32): {count}\")\n",
        "    print(f\"Number of bit positions to flip (per sample): {nums}\")\n",
        "    \n",
        "    def random_bit_positionss():\n",
        "        return random.sample(range(0, 31 * count), nums)\n",
        "    \n",
        "    pos1 = random_bit_positionss()\n",
        "    pos2 = random_bit_positionss()\n",
        "    pos3 = random_bit_positionss()\n",
        "    \n",
        "    all_positions = pos1 + pos2 + pos3\n",
        "\n",
        "    print(\"\\nSample of pos1 (first 20 values):\")\n",
        "    print(pos1[:20])\n",
        "    \n",
        "    print(\"\\nSample of pos2 (first 20 values):\")\n",
        "    print(pos2[:20])\n",
        "    \n",
        "    print(\"\\nSample of pos3 (first 20 values):\")\n",
        "    print(pos3[:20])\n",
        "    \n",
        "    print(f\"\\nTotal combined positions (len={len(all_positions)}), sample:\")\n",
        "    print(all_positions[:20])\n",
        "    print(len(pos1))\n",
        "    print(len(pos2))\n",
        "    print(len(pos3))\n",
        "    print(len(all_positions))\n",
        "\n",
        "debug_bit_positions(BER, model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Logger\n",
        "\"\"\"\n",
        "import csv\n",
        "import os\n",
        "\n",
        "csv_path = \"results.csv\"\n",
        "\n",
        "# Initialize CSV (only once)\n",
        "if not os.path.exists(csv_path):\n",
        "    with open(csv_path, \"w\", newline='') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([\"BER_power\", \"Iteration\", \"Accuracy\"])\n",
        "\n",
        "# During each run:\n",
        "def log_to_csv(power, iteration, accuracy):\n",
        "    with open(csv_path, \"a\", newline='') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([power, iteration, accuracy])\n",
        "        f.flush()\n",
        "        os.fsync(f.fileno())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 686
        },
        "id": "IWSfUbojTt-R",
        "outputId": "a9622156-261a-4f6a-eafb-f7729a6f1b21"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-6\n",
            "Positions repeated >=2 times: 1\n",
            "Positions inside valid range: 6545\n",
            "Positions satisfying both: 0\n",
            "No 2-agreement bit flips found. Nothing to do.\n",
            "[0.7475]\n",
            "-6\n",
            "Positions repeated >=2 times: 2\n",
            "Positions inside valid range: 6566\n",
            "Positions satisfying both: 0\n",
            "No 2-agreement bit flips found. Nothing to do.\n",
            "[0.7475, 0.7475]\n",
            "-5\n",
            "Positions repeated >=2 times: 45\n",
            "Positions inside valid range: 66164\n",
            "Positions satisfying both: 3\n",
            "[0.7475, 0.7475, 0.7222]\n",
            "-5\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-0e95f9464278>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mBER\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mpower\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m#fault_position,bits=fault_positions(model,BER)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mfault_tolerance_two_agree\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mBER\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mreturn_acc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreturn_pre\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreturn_rec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreturn_tacc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreturn_conf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreturn_sub_conf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreturn_acc_50\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mAccuracy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturn_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-d6cebf3e4d4c>\u001b[0m in \u001b[0;36mfault_tolerance_two_agree\u001b[0;34m(BER, model)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;31m# Debug counts after function is defined\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mcount_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpos_counts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcnt\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0mcount_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpos\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpos_counts\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_in_valid_range\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m31\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0mcount_both\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpos_counts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcnt\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_in_valid_range\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m31\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-d6cebf3e4d4c>\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;31m# Debug counts after function is defined\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mcount_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpos_counts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcnt\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0mcount_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpos\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpos_counts\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_in_valid_range\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m31\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0mcount_both\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpos_counts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcnt\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_in_valid_range\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m31\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-d6cebf3e4d4c>\u001b[0m in \u001b[0;36mis_in_valid_range\u001b[0;34m(index)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mis_in_valid_range\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mend\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalid_ranges\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;31m# Debug counts after function is defined\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-d6cebf3e4d4c>\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mis_in_valid_range\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mend\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalid_ranges\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;31m# Debug counts after function is defined\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#doual\n",
        "Accuracy=[]\n",
        "Precision=[]\n",
        "Recall=[]\n",
        "Tacc=[]\n",
        "conf=[]\n",
        "sub_conf=[]\n",
        "fault_position_array=[]\n",
        "bits_array=[]\n",
        "acc_50=[]\n",
        "M=6\n",
        "power=-6\n",
        "while (power<-4):\n",
        "  for i in range (2):\n",
        "    print(power)\n",
        "    BER=5*(10**power)\n",
        "    #fault_position,bits=fault_positions(model,BER)\n",
        "    fault_tolerance_two_agree( BER, model)\n",
        "    return_acc,return_pre,return_rec,return_tacc,return_conf,return_sub_conf,return_acc_50=test(model)\n",
        "    Accuracy.append(return_acc)\n",
        "    Precision.append(return_pre)\n",
        "    Recall.append(return_rec)\n",
        "    Tacc.append(return_tacc)\n",
        "    conf.append(return_conf)\n",
        "    sub_conf.append(return_sub_conf)\n",
        "    acc_50.append(return_acc_50)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # Use GPU if available\n",
        "    model = torch.load(path)\n",
        "    model.eval()\n",
        "    print(Accuracy)\n",
        "    log_to_csv(power, i, return_acc)\n",
        "  power+=1"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "hp_dnn",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
